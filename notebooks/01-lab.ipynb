{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873440fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "pl.Config.set_tbl_rows(n=200)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e246093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "\n",
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff29f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/Projects/End-to-end-Sale-Forecasting\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374d2f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">{</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'userId'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'sunt aut facere repellat provident occaecati excepturi optio reprehenderit'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">quas totam\\nnostrum rerum est autem sunt rem eveniet architecto'</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">}</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">{</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'userId'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'qui est esse'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">nulla'</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">}</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">{</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'userId'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ea molestias quasi exercitationem repellat qui ipsa sit aut'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut'</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">}</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;0;255;0m[\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m{\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'userId'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'id'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'title'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'sunt aut facere repellat provident occaecati excepturi optio reprehenderit'\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'body'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut \u001b[0m\n",
       "\u001b[32mquas totam\\nnostrum rerum est autem sunt rem eveniet architecto'\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m}\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m{\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'userId'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'id'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'title'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'qui est esse'\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'body'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat \u001b[0m\n",
       "\u001b[32mblanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi \u001b[0m\n",
       "\u001b[32mnulla'\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m}\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m{\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'userId'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'id'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'title'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'ea molestias quasi exercitationem repellat qui ipsa sit aut'\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'body'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel \u001b[0m\n",
       "\u001b[32maccusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut'\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m}\u001b[0m\n",
       "\u001b[1;38;2;0;255;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import httpx\n",
    "\n",
    "url: str = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "response = httpx.get(url, timeout=10)\n",
    "response.raise_for_status()  # Raise an error for bad responses\n",
    "console.print(response.json()[:3], style=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd6737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895fde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-04 16:47:06 - src.utilities.data_gen - [INFO] - Generating data for 2025-08-29\n",
      "2025-09-04 16:47:06 - src.utilities.data_gen - [INFO] - Generating data for 2025-08-30\n",
      "2025-09-04 16:47:06 - src.utilities.data_gen - [INFO] - Generating data for 2025-08-31\n",
      "2025-09-04 16:47:06 - src.utilities.data_gen - [INFO] - Generating data for 2025-09-01\n",
      "2025-09-04 16:47:06 - src.utilities.data_gen - [INFO] - Generating data for 2025-09-02\n",
      "2025-09-04 16:47:06 - src.utilities.data_gen - [INFO] - Generating data for 2025-09-03\n",
      "2025-09-04 16:47:06 - src.utilities.data_gen - [INFO] - Generating data for 2025-09-04\n",
      "2025-09-04 16:47:06 - src.utilities.data_gen - [INFO] - Generated 15 files\n",
      "2025-09-04 16:47:06 - src.utilities.data_gen - [INFO] - Sales files: 5\n",
      "2025-09-04 16:47:06 - src.utilities.data_gen - [INFO] - Output directory: ./data/sales_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sales': ['./data/sales_data/sales/year=2025/month=08/day=30/sales_2025-08-30.parquet',\n",
       "  './data/sales_data/sales/year=2025/month=08/day=31/sales_2025-08-31.parquet',\n",
       "  './data/sales_data/sales/year=2025/month=09/day=01/sales_2025-09-01.parquet',\n",
       "  './data/sales_data/sales/year=2025/month=09/day=02/sales_2025-09-02.parquet',\n",
       "  './data/sales_data/sales/year=2025/month=09/day=03/sales_2025-09-03.parquet'],\n",
       " 'inventory': ['./data/sales_data/inventory/year=2025/week=35/inventory_2025-08-31.parquet'],\n",
       " 'customer_traffic': ['./data/sales_data/customer_traffic/year=2025/month=08/day=29/traffic_2025-08-29.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=08/day=30/traffic_2025-08-30.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=08/day=31/traffic_2025-08-31.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=09/day=01/traffic_2025-09-01.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=09/day=02/traffic_2025-09-02.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=09/day=03/traffic_2025-09-03.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=09/day=04/traffic_2025-09-04.parquet'],\n",
       " 'promotions': ['./data/sales_data/promotions/promotions.parquet'],\n",
       " 'store_events': ['./data/sales_data/store_events/events.parquet']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from include.config import app_settings\n",
    "from include.utilities.data_gen import RealisticSalesDataGenerator\n",
    "\n",
    "gen_data = RealisticSalesDataGenerator(start_date=\"2025-08-29\", end_date=\"2025-09-04\", seed=123)\n",
    "file_paths: dict[str, Any] = gen_data.generate_sales_data(output_dir=\"./data/sales_data\")\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a191a6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_files = sum(len(paths) for paths in file_paths.values())\n",
    "total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3913f8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sales data from multiple files...\n",
      "Combined sales data shape: (22, 10)\n",
      "Final training data shape: (22, 13)\n",
      "Columns: ['date', 'store_id', 'product_id', 'category', 'quantity_sold', 'sales', 'cost', 'profit', 'discount_percent', 'unit_price', 'has_promotion', 'customer_traffic', 'is_holiday']\n"
     ]
    }
   ],
   "source": [
    "from include.ml.trainer import ModelTrainer\n",
    "\n",
    "print(\"Loading sales data from multiple files...\")\n",
    "sales_dfs: list[pl.DataFrame] = []\n",
    "max_files: int = 5\n",
    "skipped_sales: int = 0\n",
    "\n",
    "for i, sales_file in enumerate(file_paths[\"sales\"][:max_files]):\n",
    "    try:\n",
    "        df = pd.read_parquet(sales_file, engine=\"pyarrow\")\n",
    "        sales_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        skipped_sales += 1\n",
    "        print(f\"  Skipping unreadable sales file {sales_file}: {e}\")\n",
    "        continue\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Loaded {i + 1} files...\")\n",
    "if not sales_dfs:\n",
    "    raise ValueError(\"No readable sales parquet files were loaded; aborting training\")\n",
    "\n",
    "sales_df = pd.concat(sales_dfs, ignore_index=True)\n",
    "print(f\"Combined sales data shape: {sales_df.shape}\")\n",
    "daily_sales = (\n",
    "    sales_df.groupby([\"date\", \"store_id\", \"product_id\", \"category\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"quantity_sold\": \"sum\",\n",
    "            \"revenue\": \"sum\",\n",
    "            \"cost\": \"sum\",\n",
    "            \"profit\": \"sum\",\n",
    "            \"discount_percent\": \"mean\",\n",
    "            \"unit_price\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "daily_sales = daily_sales.rename(columns={\"revenue\": \"sales\"})\n",
    "\n",
    "if file_paths.get(\"promotions\"):\n",
    "    try:\n",
    "        promo_df = pd.read_parquet(file_paths[\"promotions\"][0], engine=\"pyarrow\")\n",
    "        promo_summary = (\n",
    "            promo_df.groupby([\"date\", \"product_id\"])[\"discount_percent\"]\n",
    "            .max()\n",
    "            .reset_index()\n",
    "        )\n",
    "        promo_summary[\"has_promotion\"] = 1\n",
    "        daily_sales = daily_sales.merge(\n",
    "            promo_summary[[\"date\", \"product_id\", \"has_promotion\"]],\n",
    "            on=[\"date\", \"product_id\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        daily_sales[\"has_promotion\"] = daily_sales[\"has_promotion\"].fillna(0)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping promotions merge due to error: {e}\")\n",
    "\n",
    "if file_paths.get(\"customer_traffic\"):\n",
    "    traffic_dfs = []\n",
    "    skipped_traffic = 0\n",
    "    for traffic_file in file_paths[\"customer_traffic\"][:10]:\n",
    "        try:\n",
    "            traffic_dfs.append(pd.read_parquet(traffic_file, engine=\"pyarrow\"))\n",
    "        except Exception as e:\n",
    "            skipped_traffic += 1\n",
    "            print(f\"  Skipping unreadable traffic file {traffic_file}: {e}\")\n",
    "    if traffic_dfs:\n",
    "        traffic_df = pd.concat(traffic_dfs, ignore_index=True)\n",
    "        traffic_summary = (\n",
    "            traffic_df.groupby([\"date\", \"store_id\"])\n",
    "            .agg({\"customer_traffic\": \"sum\", \"is_holiday\": \"max\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "        daily_sales = daily_sales.merge(\n",
    "            traffic_summary, on=[\"date\", \"store_id\"], how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No readable traffic files; skipping merge\")\n",
    "print(f\"Final training data shape: {daily_sales.shape}\")\n",
    "print(f\"Columns: {daily_sales.columns.tolist()}\")\n",
    "# trainer = ModelTrainer()\n",
    "store_daily_sales = (\n",
    "    daily_sales.groupby([\"date\", \"store_id\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"sales\": \"sum\",\n",
    "            \"quantity_sold\": \"sum\",\n",
    "            \"profit\": \"sum\",\n",
    "            \"has_promotion\": \"mean\",\n",
    "            \"customer_traffic\": \"first\",\n",
    "            \"is_holiday\": \"first\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "store_daily_sales[\"date\"] = pd.to_datetime(store_daily_sales[\"date\"])\n",
    "store_daily_sales_pl = pl.from_pandas(store_daily_sales)\n",
    "# train_df, val_df, test_df = trainer.prepare_data(\n",
    "#     store_daily_sales_pl,\n",
    "#     target_col=\"sales\",\n",
    "#     group_cols=[\"store_id\"],\n",
    "#     categorical_cols=[\"store_id\"],\n",
    "# )\n",
    "# print(\n",
    "#     f\"Train shape: {train_df.shape}, Val shape: {val_df.shape}, Test shape: {test_df.shape}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "81c22120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sales data from multiple files...\n",
      "Combined sales data shape: (22, 10)\n",
      "Final training data shape: (22, 13)\n",
      "Columns: ['date', 'store_id', 'product_id', 'category', 'quantity_sold', 'sales', 'cost', 'profit', 'discount_percent', 'unit_price', 'has_promotion', 'customer_traffic', 'is_holiday']\n"
     ]
    }
   ],
   "source": [
    "# Convert to Polars\n",
    "\n",
    "from polars.dataframe.frame import DataFrame\n",
    "\n",
    "print(\"Loading sales data from multiple files...\")\n",
    "sales_dfs: list[pl.DataFrame] = []\n",
    "max_files: int = 5\n",
    "skipped_sales: int = 0\n",
    "\n",
    "for i, sales_file in enumerate(file_paths[\"sales\"][:max_files]):\n",
    "    try:\n",
    "        df = pl.read_parquet(sales_file)\n",
    "        sales_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        skipped_sales += 1\n",
    "        print(f\"  Skipping unreadable sales file {sales_file}: {e}\")\n",
    "        continue\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Loaded {i + 1} files...\")\n",
    "if not sales_dfs:\n",
    "    raise ValueError(\"No readable sales parquet files were loaded; aborting training\")\n",
    "\n",
    "sales_df = pl.concat(sales_dfs)\n",
    "print(f\"Combined sales data shape: {sales_df.shape}\")\n",
    "daily_sales: DataFrame = (\n",
    "    sales_df.group_by([\"date\", \"store_id\", \"product_id\", \"category\"])\n",
    "    .agg(\n",
    "        pl.col(\"quantity_sold\").sum(),\n",
    "        pl.col(\"revenue\").sum().alias(\"sales\"),\n",
    "        pl.col(\"cost\").sum(),\n",
    "        pl.col(\"profit\").sum(),\n",
    "        pl.col(\"discount_percent\").mean(),\n",
    "        pl.col(\"unit_price\").mean(),\n",
    "    )\n",
    "    .sort(\"date\", \"store_id\")\n",
    ")\n",
    "\n",
    "if file_paths.get(\"promotions\"):\n",
    "    try:\n",
    "        promo_df = pl.read_parquet(file_paths[\"promotions\"][0])\n",
    "        promo_summary = (\n",
    "            promo_df.group_by([\"date\", \"product_id\"])\n",
    "            .agg(pl.col(\"discount_percent\").max())\n",
    "            .with_columns(pl.lit(1).cast(pl.Int8).alias(\"has_promotion\"))\n",
    "        )\n",
    "        daily_sales = daily_sales.join(\n",
    "            promo_summary.select([\"date\", \"product_id\", \"has_promotion\"]),\n",
    "            on=[\"date\", \"product_id\"],\n",
    "            how=\"left\",\n",
    "        ).with_columns(pl.col(\"has_promotion\").fill_null(0))\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping promotions merge due to error: {e}\")\n",
    "\n",
    "if file_paths.get(\"customer_traffic\"):\n",
    "    traffic_dfs: list[pl.DataFrame] = []\n",
    "    skipped_traffic: int = 0\n",
    "\n",
    "    for traffic_file in file_paths[\"customer_traffic\"][:10]:\n",
    "        try:\n",
    "            traffic_dfs.append(pl.read_parquet(traffic_file))\n",
    "        except Exception as e:\n",
    "            skipped_traffic += 1\n",
    "            print(f\"  Skipping unreadable traffic file {traffic_file}: {e}\")\n",
    "\n",
    "    if traffic_dfs:\n",
    "        traffic_df = pl.concat(traffic_dfs)\n",
    "        traffic_summary = traffic_df.group_by([\"date\", \"store_id\"]).agg(\n",
    "            pl.col(\"customer_traffic\").sum(), pl.col(\"is_holiday\").max()\n",
    "        )\n",
    "        daily_sales = daily_sales.join(\n",
    "            traffic_summary,\n",
    "            on=[\"date\", \"store_id\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"No readable traffic files; skipping merge\")\n",
    "print(f\"Final training data shape: {daily_sales.shape}\")\n",
    "print(f\"Columns: {daily_sales.columns}\")\n",
    "\n",
    "# trainer = ModelTrainer()\n",
    "store_daily_sales: DataFrame = (\n",
    "    daily_sales.group_by([\"date\", \"store_id\"])\n",
    "    .agg(\n",
    "        pl.col(\"sales\").sum(),\n",
    "        pl.col(\"quantity_sold\").sum(),\n",
    "        pl.col(\"profit\").sum(),\n",
    "        pl.col(\"has_promotion\").mean(),\n",
    "        pl.col(\"customer_traffic\").first(),\n",
    "        pl.col(\"is_holiday\").first(),\n",
    "    )\n",
    "    .with_columns(pl.col(\"date\").cast(pl.Date))\n",
    ")\n",
    "# train_df, val_df, test_df = trainer.prepare_data(\n",
    "#     store_daily_sales_pl,\n",
    "#     target_col=\"sales\",\n",
    "#     group_cols=[\"store_id\"],\n",
    "#     categorical_cols=[\"store_id\"],\n",
    "# )\n",
    "# print(\n",
    "#     f\"Train shape: {train_df.shape}, Val shape: {val_df.shape}, Test shape: {test_df.shape}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a5a4ce7",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>store_id</th><th>product_id</th><th>category</th><th>quantity_sold</th><th>sales</th><th>cost</th><th>profit</th><th>discount_percent</th><th>unit_price</th><th>has_promotion</th><th>customer_traffic</th><th>is_holiday</th></tr><tr><td>datetime[ns]</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i8</td><td>i64</td><td>bool</td></tr></thead><tbody><tr><td>2025-08-30 00:00:00</td><td>&quot;store_001&quot;</td><td>&quot;HOME_005&quot;</td><td>&quot;Home&quot;</td><td>1</td><td>39.0</td><td>23.4</td><td>15.6</td><td>0.0</td><td>39.0</td><td>0</td><td>1443</td><td>false</td></tr><tr><td>2025-08-30 00:00:00</td><td>&quot;store_001&quot;</td><td>&quot;SPRT_005&quot;</td><td>&quot;Sports&quot;</td><td>1</td><td>89.0</td><td>57.85</td><td>31.15</td><td>0.0</td><td>89.0</td><td>0</td><td>1443</td><td>false</td></tr><tr><td>2025-08-30 00:00:00</td><td>&quot;store_002&quot;</td><td>&quot;CLTH_001&quot;</td><td>&quot;Clothing&quot;</td><td>1</td><td>29.0</td><td>14.5</td><td>14.5</td><td>0.0</td><td>29.0</td><td>0</td><td>1195</td><td>false</td></tr><tr><td>2025-08-31 00:00:00</td><td>&quot;store_009&quot;</td><td>&quot;SPRT_001&quot;</td><td>&quot;Sports&quot;</td><td>1</td><td>29.0</td><td>13.05</td><td>15.95</td><td>0.0</td><td>29.0</td><td>0</td><td>1221</td><td>false</td></tr><tr><td>2025-08-31 00:00:00</td><td>&quot;store_009&quot;</td><td>&quot;HOME_002&quot;</td><td>&quot;Home&quot;</td><td>1</td><td>49.0</td><td>31.85</td><td>17.15</td><td>0.0</td><td>49.0</td><td>0</td><td>1221</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "┌─────┬─────┬─────┬────────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐\n",
       "│ dat ┆ sto ┆ pro ┆ catego ┆ quant ┆ sales ┆ cost  ┆ profi ┆ disco ┆ unit_ ┆ has_p ┆ custo ┆ is_ho │\n",
       "│ e   ┆ re_ ┆ duc ┆ ry     ┆ ity_s ┆ ---   ┆ ---   ┆ t     ┆ unt_p ┆ price ┆ romot ┆ mer_t ┆ liday │\n",
       "│ --- ┆ id  ┆ t_i ┆ ---    ┆ old   ┆ f64   ┆ f64   ┆ ---   ┆ ercen ┆ ---   ┆ ion   ┆ raffi ┆ ---   │\n",
       "│ dat ┆ --- ┆ d   ┆ str    ┆ ---   ┆       ┆       ┆ f64   ┆ t     ┆ f64   ┆ ---   ┆ c     ┆ bool  │\n",
       "│ eti ┆ str ┆ --- ┆        ┆ i64   ┆       ┆       ┆       ┆ ---   ┆       ┆ i8    ┆ ---   ┆       │\n",
       "│ me[ ┆     ┆ str ┆        ┆       ┆       ┆       ┆       ┆ f64   ┆       ┆       ┆ i64   ┆       │\n",
       "│ ns] ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "╞═════╪═════╪═════╪════════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╡\n",
       "│ 202 ┆ sto ┆ HOM ┆ Home   ┆ 1     ┆ 39.0  ┆ 23.4  ┆ 15.6  ┆ 0.0   ┆ 39.0  ┆ 0     ┆ 1443  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ E_0 ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 001 ┆ 05  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 0   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ SPR ┆ Sports ┆ 1     ┆ 89.0  ┆ 57.85 ┆ 31.15 ┆ 0.0   ┆ 89.0  ┆ 0     ┆ 1443  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ T_0 ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 001 ┆ 05  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 0   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ CLT ┆ Clothi ┆ 1     ┆ 29.0  ┆ 14.5  ┆ 14.5  ┆ 0.0   ┆ 29.0  ┆ 0     ┆ 1195  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ H_0 ┆ ng     ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 002 ┆ 01  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 0   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ SPR ┆ Sports ┆ 1     ┆ 29.0  ┆ 13.05 ┆ 15.95 ┆ 0.0   ┆ 29.0  ┆ 0     ┆ 1221  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ T_0 ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 009 ┆ 01  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 1   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ HOM ┆ Home   ┆ 1     ┆ 49.0  ┆ 31.85 ┆ 17.15 ┆ 0.0   ┆ 49.0  ┆ 0     ┆ 1221  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ E_0 ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 009 ┆ 02  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 1   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "└─────┴─────┴─────┴────────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>store_id</th><th>sales</th><th>quantity_sold</th><th>profit</th><th>has_promotion</th><th>customer_traffic</th><th>is_holiday</th></tr><tr><td>date</td><td>str</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>bool</td></tr></thead><tbody><tr><td>2025-09-01</td><td>&quot;store_002&quot;</td><td>408.85</td><td>9</td><td>130.67</td><td>1.0</td><td>1216</td><td>true</td></tr><tr><td>2025-08-31</td><td>&quot;store_009&quot;</td><td>107.0</td><td>3</td><td>47.6</td><td>0.0</td><td>1221</td><td>false</td></tr><tr><td>2025-08-30</td><td>&quot;store_001&quot;</td><td>128.0</td><td>2</td><td>46.75</td><td>0.0</td><td>1443</td><td>false</td></tr><tr><td>2025-09-02</td><td>&quot;store_002&quot;</td><td>53.65</td><td>2</td><td>26.1</td><td>0.5</td><td>1006</td><td>false</td></tr><tr><td>2025-08-30</td><td>&quot;store_002&quot;</td><td>29.0</td><td>1</td><td>14.5</td><td>0.0</td><td>1195</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "┌────────────┬───────────┬────────┬──────────────┬────────┬─────────────┬─────────────┬────────────┐\n",
       "│ date       ┆ store_id  ┆ sales  ┆ quantity_sol ┆ profit ┆ has_promoti ┆ customer_tr ┆ is_holiday │\n",
       "│ ---        ┆ ---       ┆ ---    ┆ d            ┆ ---    ┆ on          ┆ affic       ┆ ---        │\n",
       "│ date       ┆ str       ┆ f64    ┆ ---          ┆ f64    ┆ ---         ┆ ---         ┆ bool       │\n",
       "│            ┆           ┆        ┆ i64          ┆        ┆ f64         ┆ i64         ┆            │\n",
       "╞════════════╪═══════════╪════════╪══════════════╪════════╪═════════════╪═════════════╪════════════╡\n",
       "│ 2025-09-01 ┆ store_002 ┆ 408.85 ┆ 9            ┆ 130.67 ┆ 1.0         ┆ 1216        ┆ true       │\n",
       "│ 2025-08-31 ┆ store_009 ┆ 107.0  ┆ 3            ┆ 47.6   ┆ 0.0         ┆ 1221        ┆ false      │\n",
       "│ 2025-08-30 ┆ store_001 ┆ 128.0  ┆ 2            ┆ 46.75  ┆ 0.0         ┆ 1443        ┆ false      │\n",
       "│ 2025-09-02 ┆ store_002 ┆ 53.65  ┆ 2            ┆ 26.1   ┆ 0.5         ┆ 1006        ┆ false      │\n",
       "│ 2025-08-30 ┆ store_002 ┆ 29.0   ┆ 1            ┆ 14.5   ┆ 0.0         ┆ 1195        ┆ false      │\n",
       "└────────────┴───────────┴────────┴──────────────┴────────┴─────────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(daily_sales.head())\n",
    "store_daily_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3a12706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category</th>\n",
       "      <th>quantity_sold</th>\n",
       "      <th>sales</th>\n",
       "      <th>cost</th>\n",
       "      <th>profit</th>\n",
       "      <th>discount_percent</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>customer_traffic</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-30</td>\n",
       "      <td>store_001</td>\n",
       "      <td>HOME_005</td>\n",
       "      <td>Home</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>23.40</td>\n",
       "      <td>15.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1443</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-30</td>\n",
       "      <td>store_001</td>\n",
       "      <td>SPRT_005</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>57.85</td>\n",
       "      <td>31.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1443</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-30</td>\n",
       "      <td>store_002</td>\n",
       "      <td>CLTH_001</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>14.50</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1195</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>store_009</td>\n",
       "      <td>CLTH_001</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>14.50</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1221</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>store_009</td>\n",
       "      <td>HOME_002</td>\n",
       "      <td>Home</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "      <td>31.85</td>\n",
       "      <td>17.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1221</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   store_id product_id  category  quantity_sold  sales   cost  \\\n",
       "0 2025-08-30  store_001   HOME_005      Home              1   39.0  23.40   \n",
       "1 2025-08-30  store_001   SPRT_005    Sports              1   89.0  57.85   \n",
       "2 2025-08-30  store_002   CLTH_001  Clothing              1   29.0  14.50   \n",
       "3 2025-08-31  store_009   CLTH_001  Clothing              1   29.0  14.50   \n",
       "4 2025-08-31  store_009   HOME_002      Home              1   49.0  31.85   \n",
       "\n",
       "   profit  discount_percent  unit_price  has_promotion  customer_traffic  \\\n",
       "0   15.60               0.0        39.0            0.0              1443   \n",
       "1   31.15               0.0        89.0            0.0              1443   \n",
       "2   14.50               0.0        29.0            0.0              1195   \n",
       "3   14.50               0.0        29.0            0.0              1221   \n",
       "4   17.15               0.0        49.0            0.0              1221   \n",
       "\n",
       "   is_holiday  \n",
       "0       False  \n",
       "1       False  \n",
       "2       False  \n",
       "3       False  \n",
       "4       False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>quantity_sold</th>\n",
       "      <th>profit</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>customer_traffic</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-30</td>\n",
       "      <td>store_001</td>\n",
       "      <td>128.00</td>\n",
       "      <td>2</td>\n",
       "      <td>46.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1443</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-30</td>\n",
       "      <td>store_002</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1195</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>store_009</td>\n",
       "      <td>107.00</td>\n",
       "      <td>3</td>\n",
       "      <td>47.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1221</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_001</td>\n",
       "      <td>149.60</td>\n",
       "      <td>4</td>\n",
       "      <td>43.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1058</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_002</td>\n",
       "      <td>408.85</td>\n",
       "      <td>9</td>\n",
       "      <td>130.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   store_id   sales  quantity_sold  profit  has_promotion  \\\n",
       "0 2025-08-30  store_001  128.00              2   46.75            0.0   \n",
       "1 2025-08-30  store_002   29.00              1   14.50            0.0   \n",
       "2 2025-08-31  store_009  107.00              3   47.60            0.0   \n",
       "3 2025-09-01  store_001  149.60              4   43.35            1.0   \n",
       "4 2025-09-01  store_002  408.85              9  130.67            1.0   \n",
       "\n",
       "   customer_traffic  is_holiday  \n",
       "0              1443       False  \n",
       "1              1195       False  \n",
       "2              1221       False  \n",
       "3              1058        True  \n",
       "4              1216        True  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(daily_sales.head())\n",
    "store_daily_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c5779e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>store_id</th><th>product_id</th><th>category</th><th>quantity_sold</th><th>revenue</th><th>cost</th><th>profit</th><th>discount_percent</th><th>unit_price</th></tr><tr><td>datetime[ns]</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2025-08-30 00:00:00</td><td>&quot;store_001&quot;</td><td>&quot;HOME_005&quot;</td><td>&quot;Home&quot;</td><td>1</td><td>39.0</td><td>23.4</td><td>15.6</td><td>0.0</td><td>39.0</td></tr><tr><td>2025-08-30 00:00:00</td><td>&quot;store_001&quot;</td><td>&quot;SPRT_005&quot;</td><td>&quot;Sports&quot;</td><td>1</td><td>89.0</td><td>57.85</td><td>31.15</td><td>0.0</td><td>89.0</td></tr><tr><td>2025-08-30 00:00:00</td><td>&quot;store_002&quot;</td><td>&quot;CLTH_001&quot;</td><td>&quot;Clothing&quot;</td><td>1</td><td>29.0</td><td>14.5</td><td>14.5</td><td>0.0</td><td>29.0</td></tr><tr><td>2025-08-31 00:00:00</td><td>&quot;store_009&quot;</td><td>&quot;SPRT_001&quot;</td><td>&quot;Sports&quot;</td><td>1</td><td>29.0</td><td>13.05</td><td>15.95</td><td>0.0</td><td>29.0</td></tr><tr><td>2025-08-31 00:00:00</td><td>&quot;store_009&quot;</td><td>&quot;HOME_002&quot;</td><td>&quot;Home&quot;</td><td>1</td><td>49.0</td><td>31.85</td><td>17.15</td><td>0.0</td><td>49.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 10)\n",
       "┌──────────┬──────────┬─────────┬─────────┬─────────┬─────────┬───────┬────────┬─────────┬─────────┐\n",
       "│ date     ┆ store_id ┆ product ┆ categor ┆ quantit ┆ revenue ┆ cost  ┆ profit ┆ discoun ┆ unit_pr │\n",
       "│ ---      ┆ ---      ┆ _id     ┆ y       ┆ y_sold  ┆ ---     ┆ ---   ┆ ---    ┆ t_perce ┆ ice     │\n",
       "│ datetime ┆ str      ┆ ---     ┆ ---     ┆ ---     ┆ f64     ┆ f64   ┆ f64    ┆ nt      ┆ ---     │\n",
       "│ [ns]     ┆          ┆ str     ┆ str     ┆ i64     ┆         ┆       ┆        ┆ ---     ┆ f64     │\n",
       "│          ┆          ┆         ┆         ┆         ┆         ┆       ┆        ┆ f64     ┆         │\n",
       "╞══════════╪══════════╪═════════╪═════════╪═════════╪═════════╪═══════╪════════╪═════════╪═════════╡\n",
       "│ 2025-08- ┆ store_00 ┆ HOME_00 ┆ Home    ┆ 1       ┆ 39.0    ┆ 23.4  ┆ 15.6   ┆ 0.0     ┆ 39.0    │\n",
       "│ 30       ┆ 1        ┆ 5       ┆         ┆         ┆         ┆       ┆        ┆         ┆         │\n",
       "│ 00:00:00 ┆          ┆         ┆         ┆         ┆         ┆       ┆        ┆         ┆         │\n",
       "│ 2025-08- ┆ store_00 ┆ SPRT_00 ┆ Sports  ┆ 1       ┆ 89.0    ┆ 57.85 ┆ 31.15  ┆ 0.0     ┆ 89.0    │\n",
       "│ 30       ┆ 1        ┆ 5       ┆         ┆         ┆         ┆       ┆        ┆         ┆         │\n",
       "│ 00:00:00 ┆          ┆         ┆         ┆         ┆         ┆       ┆        ┆         ┆         │\n",
       "│ 2025-08- ┆ store_00 ┆ CLTH_00 ┆ Clothin ┆ 1       ┆ 29.0    ┆ 14.5  ┆ 14.5   ┆ 0.0     ┆ 29.0    │\n",
       "│ 30       ┆ 2        ┆ 1       ┆ g       ┆         ┆         ┆       ┆        ┆         ┆         │\n",
       "│ 00:00:00 ┆          ┆         ┆         ┆         ┆         ┆       ┆        ┆         ┆         │\n",
       "│ 2025-08- ┆ store_00 ┆ SPRT_00 ┆ Sports  ┆ 1       ┆ 29.0    ┆ 13.05 ┆ 15.95  ┆ 0.0     ┆ 29.0    │\n",
       "│ 31       ┆ 9        ┆ 1       ┆         ┆         ┆         ┆       ┆        ┆         ┆         │\n",
       "│ 00:00:00 ┆          ┆         ┆         ┆         ┆         ┆       ┆        ┆         ┆         │\n",
       "│ 2025-08- ┆ store_00 ┆ HOME_00 ┆ Home    ┆ 1       ┆ 49.0    ┆ 31.85 ┆ 17.15  ┆ 0.0     ┆ 49.0    │\n",
       "│ 31       ┆ 9        ┆ 2       ┆         ┆         ┆         ┆       ┆        ┆         ┆         │\n",
       "│ 00:00:00 ┆          ┆         ┆         ┆         ┆         ┆       ┆        ┆         ┆         │\n",
       "└──────────┴──────────┴─────────┴─────────┴─────────┴─────────┴───────┴────────┴─────────┴─────────┘"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_sales = (\n",
    "    sales_df.group_by([\"date\", \"store_id\", \"product_id\", \"category\"])\n",
    "    .agg(\n",
    "        pl.col(\"quantity_sold\").sum(),\n",
    "        pl.col(\"revenue\").sum().alias(\"sales\"),\n",
    "        pl.col(\"cost\").sum(),\n",
    "        pl.col(\"profit\").sum(),\n",
    "        pl.col(\"discount_percent\").mean(),\n",
    "        pl.col(\"unit_price\").mean(),\n",
    "    )\n",
    "    .sort(\"date\", \"store_id\")\n",
    ")\n",
    "\n",
    "daily_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24528d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category</th>\n",
       "      <th>quantity_sold</th>\n",
       "      <th>sales</th>\n",
       "      <th>cost</th>\n",
       "      <th>profit</th>\n",
       "      <th>discount_percent</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>customer_traffic</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-30</td>\n",
       "      <td>store_001</td>\n",
       "      <td>HOME_005</td>\n",
       "      <td>Home</td>\n",
       "      <td>1</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.40</td>\n",
       "      <td>15.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1443</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-30</td>\n",
       "      <td>store_001</td>\n",
       "      <td>SPRT_005</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>89.00</td>\n",
       "      <td>57.85</td>\n",
       "      <td>31.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1443</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-30</td>\n",
       "      <td>store_002</td>\n",
       "      <td>CLTH_001</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>29.00</td>\n",
       "      <td>14.50</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1195</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>store_009</td>\n",
       "      <td>CLTH_001</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>29.00</td>\n",
       "      <td>14.50</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1221</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>store_009</td>\n",
       "      <td>HOME_002</td>\n",
       "      <td>Home</td>\n",
       "      <td>1</td>\n",
       "      <td>49.00</td>\n",
       "      <td>31.85</td>\n",
       "      <td>17.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1221</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>store_009</td>\n",
       "      <td>SPRT_001</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>29.00</td>\n",
       "      <td>13.05</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1221</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_001</td>\n",
       "      <td>CLTH_001</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>24.65</td>\n",
       "      <td>14.50</td>\n",
       "      <td>10.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1058</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_001</td>\n",
       "      <td>HOME_001</td>\n",
       "      <td>Home</td>\n",
       "      <td>1</td>\n",
       "      <td>67.15</td>\n",
       "      <td>55.30</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1058</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_001</td>\n",
       "      <td>HOME_005</td>\n",
       "      <td>Home</td>\n",
       "      <td>1</td>\n",
       "      <td>33.15</td>\n",
       "      <td>23.40</td>\n",
       "      <td>9.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1058</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_001</td>\n",
       "      <td>SPRT_001</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>24.65</td>\n",
       "      <td>13.05</td>\n",
       "      <td>11.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1058</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_002</td>\n",
       "      <td>CLTH_001</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>24.65</td>\n",
       "      <td>14.50</td>\n",
       "      <td>10.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_002</td>\n",
       "      <td>CLTH_002</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>67.15</td>\n",
       "      <td>43.45</td>\n",
       "      <td>23.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_002</td>\n",
       "      <td>CLTH_004</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>75.65</td>\n",
       "      <td>46.28</td>\n",
       "      <td>29.37</td>\n",
       "      <td>0.15</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_002</td>\n",
       "      <td>HOME_001</td>\n",
       "      <td>Home</td>\n",
       "      <td>1</td>\n",
       "      <td>67.15</td>\n",
       "      <td>55.30</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_002</td>\n",
       "      <td>HOME_002</td>\n",
       "      <td>Home</td>\n",
       "      <td>1</td>\n",
       "      <td>41.65</td>\n",
       "      <td>31.85</td>\n",
       "      <td>9.80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_002</td>\n",
       "      <td>HOME_005</td>\n",
       "      <td>Home</td>\n",
       "      <td>2</td>\n",
       "      <td>66.30</td>\n",
       "      <td>46.80</td>\n",
       "      <td>19.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_002</td>\n",
       "      <td>SPRT_001</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>24.65</td>\n",
       "      <td>13.05</td>\n",
       "      <td>11.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>store_002</td>\n",
       "      <td>SPRT_002</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>41.65</td>\n",
       "      <td>26.95</td>\n",
       "      <td>14.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>store_002</td>\n",
       "      <td>CLTH_001</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>29.00</td>\n",
       "      <td>14.50</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1006</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>store_002</td>\n",
       "      <td>SPRT_001</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>24.65</td>\n",
       "      <td>13.05</td>\n",
       "      <td>11.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1006</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>store_009</td>\n",
       "      <td>SPRT_001</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>24.65</td>\n",
       "      <td>13.05</td>\n",
       "      <td>11.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>703</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>store_009</td>\n",
       "      <td>CLTH_001</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>24.65</td>\n",
       "      <td>14.50</td>\n",
       "      <td>10.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>772</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   store_id product_id  category  quantity_sold  sales   cost  \\\n",
       "0  2025-08-30  store_001   HOME_005      Home              1  39.00  23.40   \n",
       "1  2025-08-30  store_001   SPRT_005    Sports              1  89.00  57.85   \n",
       "2  2025-08-30  store_002   CLTH_001  Clothing              1  29.00  14.50   \n",
       "3  2025-08-31  store_009   CLTH_001  Clothing              1  29.00  14.50   \n",
       "4  2025-08-31  store_009   HOME_002      Home              1  49.00  31.85   \n",
       "5  2025-08-31  store_009   SPRT_001    Sports              1  29.00  13.05   \n",
       "6  2025-09-01  store_001   CLTH_001  Clothing              1  24.65  14.50   \n",
       "7  2025-09-01  store_001   HOME_001      Home              1  67.15  55.30   \n",
       "8  2025-09-01  store_001   HOME_005      Home              1  33.15  23.40   \n",
       "9  2025-09-01  store_001   SPRT_001    Sports              1  24.65  13.05   \n",
       "10 2025-09-01  store_002   CLTH_001  Clothing              1  24.65  14.50   \n",
       "11 2025-09-01  store_002   CLTH_002  Clothing              1  67.15  43.45   \n",
       "12 2025-09-01  store_002   CLTH_004  Clothing              1  75.65  46.28   \n",
       "13 2025-09-01  store_002   HOME_001      Home              1  67.15  55.30   \n",
       "14 2025-09-01  store_002   HOME_002      Home              1  41.65  31.85   \n",
       "15 2025-09-01  store_002   HOME_005      Home              2  66.30  46.80   \n",
       "16 2025-09-01  store_002   SPRT_001    Sports              1  24.65  13.05   \n",
       "17 2025-09-01  store_002   SPRT_002    Sports              1  41.65  26.95   \n",
       "18 2025-09-02  store_002   CLTH_001  Clothing              1  29.00  14.50   \n",
       "19 2025-09-02  store_002   SPRT_001    Sports              1  24.65  13.05   \n",
       "20 2025-09-02  store_009   SPRT_001    Sports              1  24.65  13.05   \n",
       "21 2025-09-03  store_009   CLTH_001  Clothing              1  24.65  14.50   \n",
       "\n",
       "    profit  discount_percent  unit_price  has_promotion  customer_traffic  \\\n",
       "0    15.60              0.00        39.0            0.0              1443   \n",
       "1    31.15              0.00        89.0            0.0              1443   \n",
       "2    14.50              0.00        29.0            0.0              1195   \n",
       "3    14.50              0.00        29.0            0.0              1221   \n",
       "4    17.15              0.00        49.0            0.0              1221   \n",
       "5    15.95              0.00        29.0            0.0              1221   \n",
       "6    10.15              0.15        29.0            1.0              1058   \n",
       "7    11.85              0.15        79.0            1.0              1058   \n",
       "8     9.75              0.15        39.0            1.0              1058   \n",
       "9    11.60              0.15        29.0            1.0              1058   \n",
       "10   10.15              0.15        29.0            1.0              1216   \n",
       "11   23.70              0.15        79.0            1.0              1216   \n",
       "12   29.37              0.15        89.0            1.0              1216   \n",
       "13   11.85              0.15        79.0            1.0              1216   \n",
       "14    9.80              0.15        49.0            1.0              1216   \n",
       "15   19.50              0.15        39.0            1.0              1216   \n",
       "16   11.60              0.15        29.0            1.0              1216   \n",
       "17   14.70              0.15        49.0            1.0              1216   \n",
       "18   14.50              0.00        29.0            0.0              1006   \n",
       "19   11.60              0.15        29.0            1.0              1006   \n",
       "20   11.60              0.15        29.0            1.0               703   \n",
       "21   10.15              0.15        29.0            1.0               772   \n",
       "\n",
       "    is_holiday  \n",
       "0        False  \n",
       "1        False  \n",
       "2        False  \n",
       "3        False  \n",
       "4        False  \n",
       "5        False  \n",
       "6         True  \n",
       "7         True  \n",
       "8         True  \n",
       "9         True  \n",
       "10        True  \n",
       "11        True  \n",
       "12        True  \n",
       "13        True  \n",
       "14        True  \n",
       "15        True  \n",
       "16        True  \n",
       "17        True  \n",
       "18       False  \n",
       "19       False  \n",
       "20       False  \n",
       "21       False  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d1bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d2d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b122004e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0851577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ed365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pl.DataFrame = pl.DataFrame(\n",
    "    data={\n",
    "        \"id\": [1, 2, 3, 4],\n",
    "        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Bob\"],\n",
    "        \"role\": [\"Engineer\", \"Manager\", \"Engineer\", \"Manager\"],\n",
    "        \"skill\": [\"Python\", \"Leadership\", \"Python\", \"Management\"],\n",
    "        \"experience\": [5, 2, 3, 3],\n",
    "        \"age\": [30, 40, 35, 34],\n",
    "        \"target\": [1, 0, 1, 1],\n",
    "    }\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f87382",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df[\"name\"].value_counts()\n",
    "mean_target = df.group_by(\"name\").agg(pl.col(\"target\").mean())\n",
    "display(mean_target)\n",
    "display(counts[\"name\"])\n",
    "for row in counts[\"name\"]:\n",
    "    print(counts.filter(pl.col(\"name\").eq(row))[\"count\"].item())\n",
    "\n",
    "counts.filter(pl.col(\"name\").eq(\"Alice\"))[\"count\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bac96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f527771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d70434e",
   "metadata": {},
   "source": [
    "### Connect To MLFlow\n",
    "\n",
    "- Set the `tracking URI` to the MLflow server.\n",
    "    - Tracking URI requires the MLflow `server address`, `port`, `S3 endpoint URL`, and `S3 credentials`.\n",
    "    - S3 credentials include `access key`, `secret key`, and `bucket name`.\n",
    "    - `MinIO` is used as a local S3-compatible storage service.\n",
    "\n",
    "- Verify the connection by listing experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf35299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force localhost configuration and debug\n",
    "RUNNING_IN_DOCKER = False\n",
    "DEFAULT_MINIO_HOST = app_settings.AWS_S3_HOST if RUNNING_IN_DOCKER else \"minio\"\n",
    "DEFAULT_MINIO_PORT = app_settings.AWS_S3_PORT\n",
    "MINIO_ENDPOINT = app_settings.mlflow_s3_endpoint_url\n",
    "# This connects to the MLflow server with PostgreSQL backend\n",
    "MLFLOW_URI = app_settings.mlflow_tracking_uri\n",
    "AWS_KEY = app_settings.AWS_ACCESS_KEY_ID\n",
    "AWS_SECRET = app_settings.AWS_SECRET_ACCESS_KEY.get_secret_value()\n",
    "AWS_REGION = app_settings.AWS_DEFAULT_REGION\n",
    "BUCKET = app_settings.AWS_S3_BUCKET\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = app_settings.AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = MINIO_ENDPOINT\n",
    "\n",
    "print(\"=== CONFIGURATION DEBUG ===\")\n",
    "print(f\"RUNNING_IN_DOCKER: {RUNNING_IN_DOCKER}\")\n",
    "print(f\"DEFAULT_MINIO_HOST: {DEFAULT_MINIO_HOST}\")\n",
    "print(f\"MINIO_ENDPOINT: {MINIO_ENDPOINT}\")\n",
    "print(f\"MLFLOW_URI: {MLFLOW_URI}\")\n",
    "print(f\"AWS_ACCESS_KEY_ID: {AWS_KEY}\")\n",
    "print(f\"BUCKET: {BUCKET}\")\n",
    "print(f\"Environment MLFLOW_S3_ENDPOINT_URL: {MINIO_ENDPOINT}\")\n",
    "print(\"=== END CONFIGURATION DEBUG ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b689ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MLflow server connection and S3 storage\n",
    "import tempfile\n",
    "import traceback\n",
    "\n",
    "import boto3\n",
    "import mlflow\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# 1) Test S3/MinIO connection\n",
    "print(\"Testing S3/MinIO connection...\")\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=AWS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET,\n",
    "    region_name=AWS_REGION,\n",
    ")\n",
    "\n",
    "try:\n",
    "    s3.head_bucket(Bucket=BUCKET)\n",
    "    print(f\"✅ Bucket '{BUCKET}' is reachable\")\n",
    "except ClientError as e:\n",
    "    print(f\"❌ S3/MinIO connection failed: {e}\")\n",
    "\n",
    "# 2) Test MLflow server connection\n",
    "print(f\"\\nTesting MLflow server connection to {MLFLOW_URI}...\")\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "print(f\"✅ MLflow tracking URI set to: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# 3) Test that MLflow uses PostgreSQL backend (not local files)\n",
    "try:\n",
    "    # This should connect to the MLflow server which uses PostgreSQL\n",
    "    experiments = mlflow.search_experiments()\n",
    "    print(f\"✅ Connected to MLflow server. Found {len(experiments)} experiments.\")\n",
    "    print(\"✅ This confirms MLflow is using the PostgreSQL backend, not local files.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect to MLflow server: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"IMPORTANT: If MLflow server is using PostgreSQL correctly,\")\n",
    "print(\"experiments and runs will be stored in the database,\")\n",
    "print(\"and artifacts will be stored in MinIO/S3.\")\n",
    "print(\"Local 'mlruns' folders should NOT be created.\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from botocore.exceptions import ClientError\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "try:\n",
    "    mlflow.set_experiment(\"notebook_quick_test\")\n",
    "    X, y = datasets.load_diabetes(return_X_y=True)\n",
    "    model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.log_param(\"alpha\", 0.1)\n",
    "        mlflow.log_param(\"l1_ratio\", 0.5)\n",
    "        mlflow.log_metric(\"dummy_score\", model.score(X, y))\n",
    "\n",
    "        # Create a small artifact file and upload\n",
    "        with tempfile.NamedTemporaryFile(\"w\", suffix=\".txt\", delete=False) as tmp:\n",
    "            tmp.write(\"mlflow artifact test\")\n",
    "            tmp_path = tmp.name\n",
    "\n",
    "        mlflow.log_artifact(tmp_path, artifact_path=\"test_artifacts\")\n",
    "        mlflow.sklearn.log_model(model, \"model\", input_example=X[:2].tolist())\n",
    "\n",
    "        # Remove temp file after logging\n",
    "        os.remove(tmp_path)\n",
    "\n",
    "        print(\"✅ Logged run id:\", run.info.run_id)\n",
    "        print(\"✅ Experiment id:\", run.info.experiment_id)\n",
    "\n",
    "    print(\"✅ MLflow logging complete — check the UI and MinIO for artifact/model.\")\n",
    "    print(\"✅ Data stored in PostgreSQL database, artifacts in MinIO S3\")\n",
    "\n",
    "except ClientError as e:\n",
    "    # boto3 ClientError can surface during artifact upload\n",
    "    print(\"❌ Boto3 ClientError during MLflow operations:\", e)\n",
    "    print(traceback.format_exc())\n",
    "    raise\n",
    "except Exception:\n",
    "    print(\"❌ Unexpected error during MLflow logging:\")\n",
    "    print(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93578dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca7341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eceaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ded3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "#\n",
    "\n",
    "# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.\n",
    "#\n",
    "# WARNING: This configuration is for local development. Do not use it in a production deployment.\n",
    "#\n",
    "# This configuration supports basic configuration using environment variables or an .env file\n",
    "# The following variables are supported:\n",
    "#\n",
    "# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.\n",
    "#                                Default: apache/airflow:3.0.6\n",
    "# AIRFLOW_UID                  - User ID in Airflow containers\n",
    "#                                Default: 50000\n",
    "# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.\n",
    "#                                Default: .\n",
    "# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode\n",
    "#\n",
    "# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).\n",
    "#                                Default: airflow\n",
    "# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).\n",
    "#                                Default: airflow\n",
    "# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.\n",
    "#                                Use this option ONLY for quick checks. Installing requirements at container\n",
    "#                                startup is done EVERY TIME the service is started.\n",
    "#                                A better way is to build a custom image or extend the official image\n",
    "#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.\n",
    "#                                Default: ''\n",
    "#\n",
    "# Feel free to modify this file to suit your needs.\n",
    "---\n",
    "x-airflow-common:\n",
    "  &airflow-common\n",
    "  # In order to add custom dependencies or upgrade provider distributions you can use your extended image.\n",
    "  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n",
    "  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n",
    "  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.0.6}\n",
    "  # build: .\n",
    "  environment:\n",
    "    &airflow-common-env\n",
    "    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n",
    "    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager\n",
    "    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n",
    "    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n",
    "    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n",
    "    AIRFLOW__CORE__FERNET_KEY: ''\n",
    "    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n",
    "    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'\n",
    "    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'\n",
    "    # yamllint disable rule:line-length\n",
    "    # Use simple http server on scheduler for health checks\n",
    "    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server\n",
    "    # yamllint enable rule:line-length\n",
    "    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'\n",
    "    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks\n",
    "    # for other purpose (development, test and especially production usage) build/extend Airflow image.\n",
    "    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n",
    "    # The following line can be used to set a custom config file, stored in the local config folder\n",
    "    AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'\n",
    "  volumes:\n",
    "    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n",
    "    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n",
    "    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n",
    "    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n",
    "  user: \"${AIRFLOW_UID:-50000}:0\"\n",
    "  depends_on:\n",
    "    &airflow-common-depends-on\n",
    "    redis:\n",
    "      condition: service_healthy\n",
    "    postgres:\n",
    "      condition: service_healthy\n",
    "\n",
    "services:\n",
    "  postgres:\n",
    "    image: postgres:13\n",
    "    environment:\n",
    "      POSTGRES_USER: airflow\n",
    "      POSTGRES_PASSWORD: airflow\n",
    "      POSTGRES_DB: airflow\n",
    "    volumes:\n",
    "      - postgres-db-volume:/var/lib/postgresql/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n",
    "      interval: 10s\n",
    "      retries: 5\n",
    "      start_period: 5s\n",
    "    restart: always\n",
    "\n",
    "  redis:\n",
    "    # Redis is limited to 7.2-bookworm due to licencing change\n",
    "    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/\n",
    "    image: redis:7.2-bookworm\n",
    "    expose:\n",
    "      - 6379\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"redis-cli\", \"ping\"]\n",
    "      interval: 10s\n",
    "      timeout: 30s\n",
    "      retries: 50\n",
    "      start_period: 30s\n",
    "    restart: always\n",
    "\n",
    "  airflow-apiserver:\n",
    "    <<: *airflow-common\n",
    "    command: api-server\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/api/v2/version\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "      start_period: 30s\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "  airflow-scheduler:\n",
    "    <<: *airflow-common\n",
    "    command: scheduler\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "      start_period: 30s\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "  airflow-dag-processor:\n",
    "    <<: *airflow-common\n",
    "    command: dag-processor\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", 'airflow jobs check --job-type DagProcessorJob --hostname \"$${HOSTNAME}\"']\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "      start_period: 30s\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "  airflow-worker:\n",
    "    <<: *airflow-common\n",
    "    command: celery worker\n",
    "    healthcheck:\n",
    "      # yamllint disable rule:line-length\n",
    "      test:\n",
    "        - \"CMD-SHELL\"\n",
    "        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\" || celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"'\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "      start_period: 30s\n",
    "    environment:\n",
    "      <<: *airflow-common-env\n",
    "      # Required to handle warm shutdown of the celery workers properly\n",
    "      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation\n",
    "      DUMB_INIT_SETSID: \"0\"\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-apiserver:\n",
    "        condition: service_healthy\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "  airflow-triggerer:\n",
    "    <<: *airflow-common\n",
    "    command: triggerer\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", 'airflow jobs check --job-type TriggererJob --hostname \"$${HOSTNAME}\"']\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "      start_period: 30s\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "  airflow-init:\n",
    "    <<: *airflow-common\n",
    "    entrypoint: /bin/bash\n",
    "    # yamllint disable rule:line-length\n",
    "    command:\n",
    "      - -c\n",
    "      - |\n",
    "        if [[ -z \"${AIRFLOW_UID}\" ]]; then\n",
    "          echo\n",
    "          echo -e \"\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\"\n",
    "          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"\n",
    "          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"\n",
    "          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"\n",
    "          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"\n",
    "          echo\n",
    "          export AIRFLOW_UID=$$(id -u)\n",
    "        fi\n",
    "        one_meg=1048576\n",
    "        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\n",
    "        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)\n",
    "        disk_available=$$(df / | tail -1 | awk '{print $$4}')\n",
    "        warning_resources=\"false\"\n",
    "        if (( mem_available < 4000 )) ; then\n",
    "          echo\n",
    "          echo -e \"\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\"\n",
    "          echo \"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"\n",
    "          echo\n",
    "          warning_resources=\"true\"\n",
    "        fi\n",
    "        if (( cpus_available < 2 )); then\n",
    "          echo\n",
    "          echo -e \"\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\"\n",
    "          echo \"At least 2 CPUs recommended. You have $${cpus_available}\"\n",
    "          echo\n",
    "          warning_resources=\"true\"\n",
    "        fi\n",
    "        if (( disk_available < one_meg * 10 )); then\n",
    "          echo\n",
    "          echo -e \"\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\"\n",
    "          echo \"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"\n",
    "          echo\n",
    "          warning_resources=\"true\"\n",
    "        fi\n",
    "        if [[ $${warning_resources} == \"true\" ]]; then\n",
    "          echo\n",
    "          echo -e \"\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\"\n",
    "          echo \"Please follow the instructions to increase amount of resources available:\"\n",
    "          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"\n",
    "          echo\n",
    "        fi\n",
    "        echo\n",
    "        echo \"Creating missing opt dirs if missing:\"\n",
    "        echo\n",
    "        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}\n",
    "        echo\n",
    "        echo \"Airflow version:\"\n",
    "        /entrypoint airflow version\n",
    "        echo\n",
    "        echo \"Files in shared volumes:\"\n",
    "        echo\n",
    "        ls -la /opt/airflow/{logs,dags,plugins,config}\n",
    "        echo\n",
    "        echo \"Running airflow config list to create default config file if missing.\"\n",
    "        echo\n",
    "        /entrypoint airflow config list >/dev/null\n",
    "        echo\n",
    "        echo \"Files in shared volumes:\"\n",
    "        echo\n",
    "        ls -la /opt/airflow/{logs,dags,plugins,config}\n",
    "        echo\n",
    "        echo \"Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0\"\n",
    "        echo\n",
    "        chown -R \"${AIRFLOW_UID}:0\" /opt/airflow/\n",
    "        echo\n",
    "        echo \"Change ownership of files in shared volumes to ${AIRFLOW_UID}:0\"\n",
    "        echo\n",
    "        chown -v -R \"${AIRFLOW_UID}:0\" /opt/airflow/{logs,dags,plugins,config}\n",
    "        echo\n",
    "        echo \"Files in shared volumes:\"\n",
    "        echo\n",
    "        ls -la /opt/airflow/{logs,dags,plugins,config}\n",
    "\n",
    "    # yamllint enable rule:line-length\n",
    "    environment:\n",
    "      <<: *airflow-common-env\n",
    "      _AIRFLOW_DB_MIGRATE: 'true'\n",
    "      _AIRFLOW_WWW_USER_CREATE: 'true'\n",
    "      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n",
    "      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n",
    "      _PIP_ADDITIONAL_REQUIREMENTS: ''\n",
    "    user: \"0:0\"\n",
    "\n",
    "  airflow-cli:\n",
    "    <<: *airflow-common\n",
    "    profiles:\n",
    "      - debug\n",
    "    environment:\n",
    "      <<: *airflow-common-env\n",
    "      CONNECTION_CHECK_MAX_COUNT: \"0\"\n",
    "    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252\n",
    "    command:\n",
    "      - bash\n",
    "      - -c\n",
    "      - airflow\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "\n",
    "  # You can enable flower by adding \"--profile flower\" option e.g. docker-compose --profile flower up\n",
    "  # or by explicitly targeted on the command line e.g. docker-compose up flower.\n",
    "  # See: https://docs.docker.com/compose/profiles/\n",
    "  flower:\n",
    "    <<: *airflow-common\n",
    "    command: celery flower\n",
    "    profiles:\n",
    "      - flower\n",
    "    ports:\n",
    "      - \"5555:5555\"\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "      start_period: 30s\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "volumes:\n",
    "  postgres-db-volume:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10909e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the cyclical features to see what's wrong\n",
    "cyclical_result = create_cyclical_features(temp_df, date_col=\"date\")\n",
    "\n",
    "# Check the cyclical features\n",
    "cyclical_sample = cyclical_result.select(\n",
    "    [\n",
    "        \"date\",\n",
    "        \"day_of_week\",\n",
    "        \"day_of_week_sin\",\n",
    "        \"day_of_week_cos\",\n",
    "        \"month_sin\",\n",
    "        \"month_cos\",\n",
    "    ]\n",
    ").unique()\n",
    "\n",
    "print(\"Cyclical features sample:\")\n",
    "print(cyclical_sample)\n",
    "\n",
    "print(\"\\nLet's check the day_of_week values and corresponding sin/cos:\")\n",
    "day_check = (\n",
    "    cyclical_result.select([\"date\", \"day_of_week\", \"day_of_week_sin\", \"day_of_week_cos\"]).unique().sort(\"day_of_week\")\n",
    ")\n",
    "print(day_check)\n",
    "\n",
    "print(\"\\nIssue Analysis:\")\n",
    "print(\"day_of_week ranges from 1-7 in Polars (Monday=1, Sunday=7)\")\n",
    "print(\"But for cyclical encoding, we want values from 0 to 2π\")\n",
    "print(\"Current formula: sin(2π × day_of_week / 7)\")\n",
    "print(\"This means day 7 gives: sin(2π × 7 / 7) = sin(2π) = 0\")\n",
    "print(\"And day 1 gives: sin(2π × 1 / 7) = sin(2π/7)\")\n",
    "print(\"This creates a discontinuity between Sunday (7) and Monday (1)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a93ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "# Add include path\n",
    "sys.path.append(\"/usr/local/airflow/include\")\n",
    "\n",
    "from ml_models.train_models import ModelTrainer\n",
    "from utils.mlflow_utils import MLflowManager\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data-team\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"start_date\": datetime(2025, 7, 22),\n",
    "    \"email_on_failure\": True,\n",
    "    \"email_on_retry\": False,\n",
    "    \"email\": [\"admin@example.com\"],\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "\n",
    "@dag(\n",
    "    schedule=\"@weekly\",\n",
    "    start_date=datetime(2025, 7, 22),\n",
    "    catchup=False,\n",
    "    default_args=default_args,\n",
    "    description=\"Train sales forecasting models\",\n",
    "    tags=[\"ml\", \"training\", \"sales\"],\n",
    ")\n",
    "def sales_forecast_training():\n",
    "    @task()\n",
    "    def extract_data_task():\n",
    "        from utils.data_generator import RealisticSalesDataGenerator\n",
    "\n",
    "        data_output_dir = \"/tmp/sales_data\"\n",
    "        generator = RealisticSalesDataGenerator(start_date=\"2021-01-01\", end_date=\"2021-12-31\")\n",
    "        print(\"Generating realistic sales data...\")\n",
    "        file_paths = generator.generate_sales_data(output_dir=data_output_dir)\n",
    "        total_files = sum(len(paths) for paths in file_paths.values())\n",
    "        print(f\"Generated {total_files} files:\")\n",
    "        for data_type, paths in file_paths.items():\n",
    "            print(f\"  - {data_type}: {len(paths)} files\")\n",
    "        return {\n",
    "            \"data_output_dir\": data_output_dir,\n",
    "            \"file_paths\": file_paths,\n",
    "            \"total_files\": total_files,\n",
    "        }\n",
    "\n",
    "    @task()\n",
    "    def validate_data_task(extract_result):\n",
    "        file_paths = extract_result[\"file_paths\"]\n",
    "        total_rows = 0\n",
    "        issues_found = []\n",
    "        print(f\"Validating {len(file_paths['sales'])} sales files...\")\n",
    "        for i, sales_file in enumerate(file_paths[\"sales\"][:10]):\n",
    "            df = pd.read_parquet(sales_file)\n",
    "            if i == 0:\n",
    "                print(f\"Sales data columns: {df.columns.tolist()}\")\n",
    "            if df.empty:\n",
    "                issues_found.append(f\"Empty file: {sales_file}\")\n",
    "                continue\n",
    "            required_cols = [\n",
    "                \"date\",\n",
    "                \"store_id\",\n",
    "                \"product_id\",\n",
    "                \"quantity_sold\",\n",
    "                \"revenue\",\n",
    "            ]\n",
    "            missing_cols = set(required_cols) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                issues_found.append(f\"Missing columns in {sales_file}: {missing_cols}\")\n",
    "            total_rows += len(df)\n",
    "            if df[\"quantity_sold\"].min() < 0:\n",
    "                issues_found.append(f\"Negative quantities in {sales_file}\")\n",
    "            if df[\"revenue\"].min() < 0:\n",
    "                issues_found.append(f\"Negative revenue in {sales_file}\")\n",
    "        for data_type in [\"promotions\", \"store_events\", \"customer_traffic\"]:\n",
    "            if data_type in file_paths and file_paths[data_type]:\n",
    "                sample_file = file_paths[data_type][0]\n",
    "                df = pd.read_parquet(sample_file)\n",
    "                print(f\"{data_type} data shape: {df.shape}\")\n",
    "                print(f\"{data_type} columns: {df.columns.tolist()}\")\n",
    "        validation_summary = {\n",
    "            \"total_files_validated\": len(file_paths[\"sales\"][:10]),\n",
    "            \"total_rows\": total_rows,\n",
    "            \"issues_found\": len(issues_found),\n",
    "            \"issues\": issues_found[:5],\n",
    "        }\n",
    "        if issues_found:\n",
    "            print(f\"Validation completed with {len(issues_found)} issues:\")\n",
    "            for issue in issues_found[:5]:\n",
    "                print(f\"  - {issue}\")\n",
    "        else:\n",
    "            print(f\"Validation passed! Total rows: {total_rows}\")\n",
    "        return validation_summary\n",
    "\n",
    "    @task()\n",
    "    def train_models_task(extract_result, validation_summary):\n",
    "        file_paths = extract_result[\"file_paths\"]\n",
    "        print(\"Loading sales data from multiple files...\")\n",
    "        sales_dfs = []\n",
    "        max_files = 50\n",
    "        for i, sales_file in enumerate(file_paths[\"sales\"][:max_files]):\n",
    "            df = pd.read_parquet(sales_file)\n",
    "            sales_dfs.append(df)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Loaded {i + 1} files...\")\n",
    "        sales_df = pd.concat(sales_dfs, ignore_index=True)\n",
    "        print(f\"Combined sales data shape: {sales_df.shape}\")\n",
    "        daily_sales = (\n",
    "            sales_df.groupby([\"date\", \"store_id\", \"product_id\", \"category\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"quantity_sold\": \"sum\",\n",
    "                    \"revenue\": \"sum\",\n",
    "                    \"cost\": \"sum\",\n",
    "                    \"profit\": \"sum\",\n",
    "                    \"discount_percent\": \"mean\",\n",
    "                    \"unit_price\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        daily_sales = daily_sales.rename(columns={\"revenue\": \"sales\"})\n",
    "        if file_paths.get(\"promotions\"):\n",
    "            promo_df = pd.read_parquet(file_paths[\"promotions\"][0])\n",
    "            promo_summary = promo_df.groupby([\"date\", \"product_id\"])[\"discount_percent\"].max().reset_index()\n",
    "            promo_summary[\"has_promotion\"] = 1\n",
    "            daily_sales = daily_sales.merge(\n",
    "                promo_summary[[\"date\", \"product_id\", \"has_promotion\"]],\n",
    "                on=[\"date\", \"product_id\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            daily_sales[\"has_promotion\"] = daily_sales[\"has_promotion\"].fillna(0)\n",
    "        if file_paths.get(\"customer_traffic\"):\n",
    "            traffic_dfs = []\n",
    "            for traffic_file in file_paths[\"customer_traffic\"][:10]:\n",
    "                traffic_dfs.append(pd.read_parquet(traffic_file))\n",
    "            traffic_df = pd.concat(traffic_dfs, ignore_index=True)\n",
    "            traffic_summary = (\n",
    "                traffic_df.groupby([\"date\", \"store_id\"]).agg({\"customer_traffic\": \"sum\", \"is_holiday\": \"max\"}).reset_index()\n",
    "            )\n",
    "            daily_sales = daily_sales.merge(traffic_summary, on=[\"date\", \"store_id\"], how=\"left\")\n",
    "        print(f\"Final training data shape: {daily_sales.shape}\")\n",
    "        print(f\"Columns: {daily_sales.columns.tolist()}\")\n",
    "        trainer = ModelTrainer()\n",
    "        store_daily_sales = (\n",
    "            daily_sales.groupby([\"date\", \"store_id\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"sales\": \"sum\",\n",
    "                    \"quantity_sold\": \"sum\",\n",
    "                    \"profit\": \"sum\",\n",
    "                    \"has_promotion\": \"mean\",\n",
    "                    \"customer_traffic\": \"first\",\n",
    "                    \"is_holiday\": \"first\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        store_daily_sales[\"date\"] = pd.to_datetime(store_daily_sales[\"date\"])\n",
    "        train_df, val_df, test_df = trainer.prepare_data(\n",
    "            store_daily_sales,\n",
    "            target_col=\"sales\",\n",
    "            date_col=\"date\",\n",
    "            group_cols=[\"store_id\"],\n",
    "            categorical_cols=[\"store_id\"],\n",
    "        )\n",
    "        print(f\"Train shape: {train_df.shape}, Val shape: {val_df.shape}, Test shape: {test_df.shape}\")\n",
    "        results = trainer.train_all_models(train_df, val_df, test_df, target_col=\"sales\", use_optuna=True)\n",
    "        for model_name, model_results in results.items():\n",
    "            if \"metrics\" in model_results:\n",
    "                print(f\"\\n{model_name} metrics:\")\n",
    "                for metric, value in model_results[\"metrics\"].items():\n",
    "                    print(f\"  {metric}: {value:.4f}\")\n",
    "        print(\"\\nVisualization charts have been generated and saved to MLflow/MinIO\")\n",
    "        print(\"Charts include:\")\n",
    "        print(\"  - Model metrics comparison\")\n",
    "        print(\"  - Predictions vs actual values\")\n",
    "        print(\"  - Residuals analysis\")\n",
    "        print(\"  - Error distribution\")\n",
    "        print(\"  - Feature importance comparison\")\n",
    "        serializable_results = {}\n",
    "        for model_name, model_results in results.items():\n",
    "            serializable_results[model_name] = {\"metrics\": model_results.get(\"metrics\", {})}\n",
    "        import mlflow\n",
    "\n",
    "        current_run_id = mlflow.active_run().info.run_id if mlflow.active_run() else None\n",
    "        return {\n",
    "            \"training_results\": serializable_results,\n",
    "            \"mlflow_run_id\": current_run_id,\n",
    "        }\n",
    "\n",
    "    @task()\n",
    "    def evaluate_models_task(training_result):\n",
    "        results = training_result[\"training_results\"]\n",
    "        mlflow_manager = MLflowManager()\n",
    "        best_model_name = None\n",
    "        best_rmse = float(\"inf\")\n",
    "        for model_name, model_results in results.items():\n",
    "            if \"metrics\" in model_results and \"rmse\" in model_results[\"metrics\"]:\n",
    "                if model_results[\"metrics\"][\"rmse\"] < best_rmse:\n",
    "                    best_rmse = model_results[\"metrics\"][\"rmse\"]\n",
    "                    best_model_name = model_name\n",
    "        print(f\"Best model: {best_model_name} with RMSE: {best_rmse:.4f}\")\n",
    "        best_run = mlflow_manager.get_best_model(metric=\"rmse\", ascending=True)\n",
    "        return {\"best_model\": best_model_name, \"best_run_id\": best_run[\"run_id\"]}\n",
    "\n",
    "    @task()\n",
    "    def register_best_model_task(evaluation_result):\n",
    "        evaluation_result[\"best_model\"]\n",
    "        run_id = evaluation_result[\"best_run_id\"]\n",
    "        mlflow_manager = MLflowManager()\n",
    "        model_versions = {}\n",
    "        for model_name in [\"xgboost\", \"lightgbm\"]:\n",
    "            version = mlflow_manager.register_model(run_id, model_name, model_name)\n",
    "            model_versions[model_name] = version\n",
    "            print(f\"Registered {model_name} version: {version}\")\n",
    "        return model_versions\n",
    "\n",
    "    @task()\n",
    "    def transition_to_production_task(model_versions):\n",
    "        mlflow_manager = MLflowManager()\n",
    "        for model_name, version in model_versions.items():\n",
    "            mlflow_manager.transition_model_stage(model_name, version, \"Production\")\n",
    "            print(f\"Transitioned {model_name} v{version} to Production\")\n",
    "        return \"Models transitioned to production\"\n",
    "\n",
    "    @task()\n",
    "    def generate_performance_report_task(training_result, validation_summary):\n",
    "        results = training_result[\"training_results\"]\n",
    "        report = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_summary\": {\n",
    "                \"total_rows\": (validation_summary.get(\"total_rows\", 0) if validation_summary else 0),\n",
    "                \"files_validated\": (validation_summary.get(\"total_files_validated\", 0) if validation_summary else 0),\n",
    "                \"issues_found\": (validation_summary.get(\"issues_found\", 0) if validation_summary else 0),\n",
    "                \"issues\": (validation_summary.get(\"issues\", []) if validation_summary else []),\n",
    "            },\n",
    "            \"model_performance\": {},\n",
    "        }\n",
    "        if results:\n",
    "            for model_name, model_results in results.items():\n",
    "                if \"metrics\" in model_results:\n",
    "                    report[\"model_performance\"][model_name] = model_results[\"metrics\"]\n",
    "        import json\n",
    "\n",
    "        with open(\"/tmp/performance_report.json\", \"w\") as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        print(\"Performance report generated\")\n",
    "        print(f\"Models trained: {list(report['model_performance'].keys())}\")\n",
    "        return report\n",
    "\n",
    "    # Task dependencies using function calls\n",
    "    extract_result = extract_data_task()\n",
    "    validation_summary = validate_data_task(extract_result)\n",
    "    training_result = train_models_task(extract_result, validation_summary)\n",
    "    evaluation_result = evaluate_models_task(training_result)\n",
    "    model_versions = register_best_model_task(evaluation_result)\n",
    "    transition = transition_to_production_task(model_versions)\n",
    "    report = generate_performance_report_task(training_result, validation_summary)\n",
    "    cleanup = BashOperator(\n",
    "        task_id=\"cleanup\",\n",
    "        bash_command=\"rm -rf /tmp/sales_data /tmp/performance_report.json || true\",\n",
    "    )\n",
    "    report >> cleanup\n",
    "\n",
    "\n",
    "sales_forecast_training_dag = sales_forecast_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db62b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "-2.4493e-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eafaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "End-to-end-Sale-Forecasting (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
