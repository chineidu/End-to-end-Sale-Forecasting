{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873440fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "pl.Config.set_tbl_rows(n=200)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e246093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "\n",
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff29f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/Projects/End-to-end-Sale-Forecasting\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374d2f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">{</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'userId'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'sunt aut facere repellat provident occaecati excepturi optio reprehenderit'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">quas totam\\nnostrum rerum est autem sunt rem eveniet architecto'</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">}</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">{</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'userId'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'qui est esse'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">nulla'</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">}</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">{</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'userId'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ea molestias quasi exercitationem repellat qui ipsa sit aut'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">,</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut'</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">    </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">}</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;0;255;0m[\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m{\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'userId'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'id'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'title'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'sunt aut facere repellat provident occaecati excepturi optio reprehenderit'\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'body'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut \u001b[0m\n",
       "\u001b[32mquas totam\\nnostrum rerum est autem sunt rem eveniet architecto'\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m}\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m{\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'userId'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'id'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'title'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'qui est esse'\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'body'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat \u001b[0m\n",
       "\u001b[32mblanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi \u001b[0m\n",
       "\u001b[32mnulla'\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m}\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m{\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'userId'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'id'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'title'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'ea molestias quasi exercitationem repellat qui ipsa sit aut'\u001b[0m\u001b[38;2;0;255;0m,\u001b[0m\n",
       "\u001b[38;2;0;255;0m        \u001b[0m\u001b[32m'body'\u001b[0m\u001b[38;2;0;255;0m: \u001b[0m\u001b[32m'et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel \u001b[0m\n",
       "\u001b[32maccusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut'\u001b[0m\n",
       "\u001b[38;2;0;255;0m    \u001b[0m\u001b[1;38;2;0;255;0m}\u001b[0m\n",
       "\u001b[1;38;2;0;255;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import httpx\n",
    "\n",
    "url: str = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "response = httpx.get(url, timeout=10)\n",
    "response.raise_for_status()  # Raise an error for bad responses\n",
    "console.print(response.json()[:3], style=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd6737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f895fde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-04 23:36:04 - include.utilities.data_gen - [INFO] - Generating data for 2025-08-29\n",
      "2025-09-04 23:36:04 - include.utilities.data_gen - [INFO] - Generating data for 2025-08-30\n",
      "2025-09-04 23:36:04 - include.utilities.data_gen - [INFO] - Generating data for 2025-08-31\n",
      "2025-09-04 23:36:04 - include.utilities.data_gen - [INFO] - Generating data for 2025-09-01\n",
      "2025-09-04 23:36:04 - include.utilities.data_gen - [INFO] - Generating data for 2025-09-02\n",
      "2025-09-04 23:36:04 - include.utilities.data_gen - [INFO] - Generating data for 2025-09-03\n",
      "2025-09-04 23:36:04 - include.utilities.data_gen - [INFO] - Generating data for 2025-09-04\n",
      "2025-09-04 23:36:04 - include.utilities.data_gen - [INFO] - Generated 15 files\n",
      "2025-09-04 23:36:04 - include.utilities.data_gen - [INFO] - Sales files: 5\n",
      "2025-09-04 23:36:04 - include.utilities.data_gen - [INFO] - Output directory: ./data/sales_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sales': ['./data/sales_data/sales/year=2025/month=08/day=30/sales_2025-08-30.parquet',\n",
       "  './data/sales_data/sales/year=2025/month=08/day=31/sales_2025-08-31.parquet',\n",
       "  './data/sales_data/sales/year=2025/month=09/day=01/sales_2025-09-01.parquet',\n",
       "  './data/sales_data/sales/year=2025/month=09/day=02/sales_2025-09-02.parquet',\n",
       "  './data/sales_data/sales/year=2025/month=09/day=03/sales_2025-09-03.parquet'],\n",
       " 'inventory': ['./data/sales_data/inventory/year=2025/week=35/inventory_2025-08-31.parquet'],\n",
       " 'customer_traffic': ['./data/sales_data/customer_traffic/year=2025/month=08/day=29/traffic_2025-08-29.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=08/day=30/traffic_2025-08-30.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=08/day=31/traffic_2025-08-31.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=09/day=01/traffic_2025-09-01.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=09/day=02/traffic_2025-09-02.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=09/day=03/traffic_2025-09-03.parquet',\n",
       "  './data/sales_data/customer_traffic/year=2025/month=09/day=04/traffic_2025-09-04.parquet'],\n",
       " 'promotions': ['./data/sales_data/promotions/promotions.parquet'],\n",
       " 'store_events': ['./data/sales_data/store_events/events.parquet']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from include.config import app_settings\n",
    "from include.utilities.data_gen import RealisticSalesDataGenerator\n",
    "\n",
    "gen_data = RealisticSalesDataGenerator(\n",
    "    start_date=\"2025-08-29\", end_date=\"2025-09-04\", seed=123\n",
    ")\n",
    "file_paths: dict[str, Any] = gen_data.generate_sales_data(\n",
    "    output_dir=\"./data/sales_data\"\n",
    ")\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a191a6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_files = sum(len(paths) for paths in file_paths.values())\n",
    "total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3913f8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sales data from multiple files...\n",
      "Combined sales data shape: (14, 10)\n",
      "Final training data shape: (14, 13)\n",
      "Columns: ['date', 'store_id', 'product_id', 'category', 'quantity_sold', 'sales', 'cost', 'profit', 'discount_percent', 'unit_price', 'has_promotion', 'customer_traffic', 'is_holiday']\n"
     ]
    }
   ],
   "source": [
    "from include.ml.trainer import ModelTrainer\n",
    "\n",
    "print(\"Loading sales data from multiple files...\")\n",
    "sales_dfs: list[pl.DataFrame] = []\n",
    "max_files: int = 5\n",
    "skipped_sales: int = 0\n",
    "\n",
    "for i, sales_file in enumerate(file_paths[\"sales\"][:max_files]):\n",
    "    try:\n",
    "        df = pd.read_parquet(sales_file, engine=\"pyarrow\")\n",
    "        sales_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        skipped_sales += 1\n",
    "        print(f\"  Skipping unreadable sales file {sales_file}: {e}\")\n",
    "        continue\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Loaded {i + 1} files...\")\n",
    "if not sales_dfs:\n",
    "    raise ValueError(\"No readable sales parquet files were loaded; aborting training\")\n",
    "\n",
    "sales_df = pd.concat(sales_dfs, ignore_index=True)\n",
    "print(f\"Combined sales data shape: {sales_df.shape}\")\n",
    "daily_sales = (\n",
    "    sales_df.groupby([\"date\", \"store_id\", \"product_id\", \"category\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"quantity_sold\": \"sum\",\n",
    "            \"revenue\": \"sum\",\n",
    "            \"cost\": \"sum\",\n",
    "            \"profit\": \"sum\",\n",
    "            \"discount_percent\": \"mean\",\n",
    "            \"unit_price\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "daily_sales = daily_sales.rename(columns={\"revenue\": \"sales\"})\n",
    "\n",
    "if file_paths.get(\"promotions\"):\n",
    "    try:\n",
    "        promo_df = pd.read_parquet(file_paths[\"promotions\"][0], engine=\"pyarrow\")\n",
    "        promo_summary = (\n",
    "            promo_df.groupby([\"date\", \"product_id\"])[\"discount_percent\"]\n",
    "            .max()\n",
    "            .reset_index()\n",
    "        )\n",
    "        promo_summary[\"has_promotion\"] = 1\n",
    "        daily_sales = daily_sales.merge(\n",
    "            promo_summary[[\"date\", \"product_id\", \"has_promotion\"]],\n",
    "            on=[\"date\", \"product_id\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        daily_sales[\"has_promotion\"] = daily_sales[\"has_promotion\"].fillna(0)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping promotions merge due to error: {e}\")\n",
    "\n",
    "if file_paths.get(\"customer_traffic\"):\n",
    "    traffic_dfs = []\n",
    "    skipped_traffic = 0\n",
    "    for traffic_file in file_paths[\"customer_traffic\"][:10]:\n",
    "        try:\n",
    "            traffic_dfs.append(pd.read_parquet(traffic_file, engine=\"pyarrow\"))\n",
    "        except Exception as e:\n",
    "            skipped_traffic += 1\n",
    "            print(f\"  Skipping unreadable traffic file {traffic_file}: {e}\")\n",
    "    if traffic_dfs:\n",
    "        traffic_df = pd.concat(traffic_dfs, ignore_index=True)\n",
    "        traffic_summary = (\n",
    "            traffic_df.groupby([\"date\", \"store_id\"])\n",
    "            .agg({\"customer_traffic\": \"sum\", \"is_holiday\": \"max\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "        daily_sales = daily_sales.merge(\n",
    "            traffic_summary, on=[\"date\", \"store_id\"], how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No readable traffic files; skipping merge\")\n",
    "print(f\"Final training data shape: {daily_sales.shape}\")\n",
    "print(f\"Columns: {daily_sales.columns.tolist()}\")\n",
    "# trainer = ModelTrainer()\n",
    "store_daily_sales = (\n",
    "    daily_sales.groupby([\"date\", \"store_id\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"sales\": \"sum\",\n",
    "            \"quantity_sold\": \"sum\",\n",
    "            \"profit\": \"sum\",\n",
    "            \"has_promotion\": \"mean\",\n",
    "            \"customer_traffic\": \"first\",\n",
    "            \"is_holiday\": \"first\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "store_daily_sales[\"date\"] = pd.to_datetime(store_daily_sales[\"date\"])\n",
    "store_daily_sales_pl = pl.from_pandas(store_daily_sales)\n",
    "# train_df, val_df, test_df = trainer.prepare_data(\n",
    "#     store_daily_sales_pl,\n",
    "#     target_col=\"sales\",\n",
    "#     group_cols=[\"store_id\"],\n",
    "#     categorical_cols=[\"store_id\"],\n",
    "# )\n",
    "# print(\n",
    "#     f\"Train shape: {train_df.shape}, Val shape: {val_df.shape}, Test shape: {test_df.shape}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c22120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sales data from multiple files...\n",
      "Combined sales data shape: (14, 10)\n",
      "Final training data shape: (14, 13)\n",
      "Columns: ['date', 'store_id', 'product_id', 'category', 'quantity_sold', 'sales', 'cost', 'profit', 'discount_percent', 'unit_price', 'has_promotion', 'customer_traffic', 'is_holiday']\n",
      "WARNING: No regex match found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/04 23:36:20 INFO mlflow.tracking.fluent: Experiment with name 'sales_forecasting' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-04 23:36:20 - include.utilities.feature_engineering - [INFO] - Starting feature engineering pipeline\n",
      "2025-09-04 23:36:20 - include.utilities.feature_engineering - [INFO] - Created 7 lag features\n",
      "2025-09-04 23:36:20 - include.utilities.feature_engineering - [INFO] - Feature engineering pipeline completed. 41 total features.\n",
      "2025-09-04 23:36:20 - include.ml.trainer - [INFO] - Data split - {\"train_size\": 6, \"validation_size\": 0, \"test_size\": 3}\n",
      "Train shape: (6, 41), Val shape: (0, 41), Test shape: (3, 41)\n"
     ]
    }
   ],
   "source": [
    "# Convert to Polars\n",
    "\n",
    "from polars.dataframe.frame import DataFrame\n",
    "\n",
    "print(\"Loading sales data from multiple files...\")\n",
    "sales_dfs: list[pl.DataFrame] = []\n",
    "max_files: int = 5\n",
    "skipped_sales: int = 0\n",
    "\n",
    "for i, sales_file in enumerate(file_paths[\"sales\"][:max_files]):\n",
    "    try:\n",
    "        df = pl.read_parquet(sales_file)\n",
    "        sales_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        skipped_sales += 1\n",
    "        print(f\"  Skipping unreadable sales file {sales_file}: {e}\")\n",
    "        continue\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Loaded {i + 1} files...\")\n",
    "if not sales_dfs:\n",
    "    raise ValueError(\"No readable sales parquet files were loaded; aborting training\")\n",
    "\n",
    "sales_df = pl.concat(sales_dfs)\n",
    "print(f\"Combined sales data shape: {sales_df.shape}\")\n",
    "daily_sales: DataFrame = (\n",
    "    sales_df.group_by([\"date\", \"store_id\", \"product_id\", \"category\"])\n",
    "    .agg(\n",
    "        pl.col(\"quantity_sold\").sum(),\n",
    "        pl.col(\"revenue\").sum().alias(\"sales\"),\n",
    "        pl.col(\"cost\").sum(),\n",
    "        pl.col(\"profit\").sum(),\n",
    "        pl.col(\"discount_percent\").mean(),\n",
    "        pl.col(\"unit_price\").mean(),\n",
    "    )\n",
    "    .sort(\"date\", \"store_id\")\n",
    ")\n",
    "\n",
    "if file_paths.get(\"promotions\"):\n",
    "    try:\n",
    "        promo_df = pl.read_parquet(file_paths[\"promotions\"][0])\n",
    "        promo_summary = (\n",
    "            promo_df.group_by([\"date\", \"product_id\"])\n",
    "            .agg(pl.col(\"discount_percent\").max())\n",
    "            .with_columns(pl.lit(1).cast(pl.Int8).alias(\"has_promotion\"))\n",
    "        )\n",
    "        daily_sales = daily_sales.join(\n",
    "            promo_summary.select([\"date\", \"product_id\", \"has_promotion\"]),\n",
    "            on=[\"date\", \"product_id\"],\n",
    "            how=\"left\",\n",
    "        ).with_columns(pl.col(\"has_promotion\").fill_null(0))\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping promotions merge due to error: {e}\")\n",
    "\n",
    "if file_paths.get(\"customer_traffic\"):\n",
    "    traffic_dfs: list[pl.DataFrame] = []\n",
    "    skipped_traffic: int = 0\n",
    "\n",
    "    for traffic_file in file_paths[\"customer_traffic\"][:10]:\n",
    "        try:\n",
    "            traffic_dfs.append(pl.read_parquet(traffic_file))\n",
    "        except Exception as e:\n",
    "            skipped_traffic += 1\n",
    "            print(f\"  Skipping unreadable traffic file {traffic_file}: {e}\")\n",
    "\n",
    "    if traffic_dfs:\n",
    "        traffic_df = pl.concat(traffic_dfs)\n",
    "        traffic_summary = traffic_df.group_by([\"date\", \"store_id\"]).agg(\n",
    "            pl.col(\"customer_traffic\").sum(), pl.col(\"is_holiday\").max()\n",
    "        )\n",
    "        daily_sales = daily_sales.join(\n",
    "            traffic_summary,\n",
    "            on=[\"date\", \"store_id\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"No readable traffic files; skipping merge\")\n",
    "print(f\"Final training data shape: {daily_sales.shape}\")\n",
    "print(f\"Columns: {daily_sales.columns}\")\n",
    "\n",
    "trainer = ModelTrainer()\n",
    "store_daily_sales: DataFrame = (\n",
    "    daily_sales.group_by([\"date\", \"store_id\"])\n",
    "    .agg(\n",
    "        pl.col(\"sales\").sum(),\n",
    "        pl.col(\"quantity_sold\").sum(),\n",
    "        pl.col(\"profit\").sum(),\n",
    "        pl.col(\"has_promotion\").mean(),\n",
    "        pl.col(\"customer_traffic\").first(),\n",
    "        pl.col(\"is_holiday\").first(),\n",
    "    )\n",
    "    .with_columns(pl.col(\"date\").cast(pl.Date))\n",
    ")\n",
    "train_df, val_df, test_df = trainer.prepare_data(\n",
    "    store_daily_sales_pl,\n",
    "    target_col=\"sales\",\n",
    "    group_cols=[\"store_id\"],\n",
    "    categorical_cols=[\"store_id\"],\n",
    ")\n",
    "print(\n",
    "    f\"Train shape: {train_df.shape}, Val shape: {val_df.shape}, Test shape: {test_df.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5a4ce7",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>store_id</th><th>product_id</th><th>category</th><th>quantity_sold</th><th>sales</th><th>cost</th><th>profit</th><th>discount_percent</th><th>unit_price</th><th>has_promotion</th><th>customer_traffic</th><th>is_holiday</th></tr><tr><td>datetime[ns]</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i8</td><td>i64</td><td>bool</td></tr></thead><tbody><tr><td>2025-08-30 00:00:00</td><td>&quot;store_001&quot;</td><td>&quot;HOME_005&quot;</td><td>&quot;Home&quot;</td><td>1</td><td>39.0</td><td>23.4</td><td>15.6</td><td>0.0</td><td>39.0</td><td>0</td><td>1443</td><td>false</td></tr><tr><td>2025-08-30 00:00:00</td><td>&quot;store_001&quot;</td><td>&quot;SPRT_005&quot;</td><td>&quot;Sports&quot;</td><td>1</td><td>89.0</td><td>57.85</td><td>31.15</td><td>0.0</td><td>89.0</td><td>0</td><td>1443</td><td>false</td></tr><tr><td>2025-08-30 00:00:00</td><td>&quot;store_002&quot;</td><td>&quot;CLTH_001&quot;</td><td>&quot;Clothing&quot;</td><td>1</td><td>29.0</td><td>14.5</td><td>14.5</td><td>0.0</td><td>29.0</td><td>0</td><td>1195</td><td>false</td></tr><tr><td>2025-08-31 00:00:00</td><td>&quot;store_009&quot;</td><td>&quot;SPRT_001&quot;</td><td>&quot;Sports&quot;</td><td>1</td><td>29.0</td><td>13.05</td><td>15.95</td><td>0.0</td><td>29.0</td><td>0</td><td>1221</td><td>false</td></tr><tr><td>2025-08-31 00:00:00</td><td>&quot;store_009&quot;</td><td>&quot;CLTH_001&quot;</td><td>&quot;Clothing&quot;</td><td>1</td><td>29.0</td><td>14.5</td><td>14.5</td><td>0.0</td><td>29.0</td><td>0</td><td>1221</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "┌─────┬─────┬─────┬────────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐\n",
       "│ dat ┆ sto ┆ pro ┆ catego ┆ quant ┆ sales ┆ cost  ┆ profi ┆ disco ┆ unit_ ┆ has_p ┆ custo ┆ is_ho │\n",
       "│ e   ┆ re_ ┆ duc ┆ ry     ┆ ity_s ┆ ---   ┆ ---   ┆ t     ┆ unt_p ┆ price ┆ romot ┆ mer_t ┆ liday │\n",
       "│ --- ┆ id  ┆ t_i ┆ ---    ┆ old   ┆ f64   ┆ f64   ┆ ---   ┆ ercen ┆ ---   ┆ ion   ┆ raffi ┆ ---   │\n",
       "│ dat ┆ --- ┆ d   ┆ str    ┆ ---   ┆       ┆       ┆ f64   ┆ t     ┆ f64   ┆ ---   ┆ c     ┆ bool  │\n",
       "│ eti ┆ str ┆ --- ┆        ┆ i64   ┆       ┆       ┆       ┆ ---   ┆       ┆ i8    ┆ ---   ┆       │\n",
       "│ me[ ┆     ┆ str ┆        ┆       ┆       ┆       ┆       ┆ f64   ┆       ┆       ┆ i64   ┆       │\n",
       "│ ns] ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "╞═════╪═════╪═════╪════════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╡\n",
       "│ 202 ┆ sto ┆ HOM ┆ Home   ┆ 1     ┆ 39.0  ┆ 23.4  ┆ 15.6  ┆ 0.0   ┆ 39.0  ┆ 0     ┆ 1443  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ E_0 ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 001 ┆ 05  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 0   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ SPR ┆ Sports ┆ 1     ┆ 89.0  ┆ 57.85 ┆ 31.15 ┆ 0.0   ┆ 89.0  ┆ 0     ┆ 1443  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ T_0 ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 001 ┆ 05  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 0   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ CLT ┆ Clothi ┆ 1     ┆ 29.0  ┆ 14.5  ┆ 14.5  ┆ 0.0   ┆ 29.0  ┆ 0     ┆ 1195  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ H_0 ┆ ng     ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 002 ┆ 01  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 0   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ SPR ┆ Sports ┆ 1     ┆ 29.0  ┆ 13.05 ┆ 15.95 ┆ 0.0   ┆ 29.0  ┆ 0     ┆ 1221  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ T_0 ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 009 ┆ 01  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 1   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ CLT ┆ Clothi ┆ 1     ┆ 29.0  ┆ 14.5  ┆ 14.5  ┆ 0.0   ┆ 29.0  ┆ 0     ┆ 1221  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ H_0 ┆ ng     ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 009 ┆ 01  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 1   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "└─────┴─────┴─────┴────────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>store_id</th><th>sales</th><th>quantity_sold</th><th>profit</th><th>has_promotion</th><th>customer_traffic</th><th>is_holiday</th></tr><tr><td>date</td><td>str</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>bool</td></tr></thead><tbody><tr><td>2025-08-30</td><td>&quot;store_002&quot;</td><td>29.0</td><td>1</td><td>14.5</td><td>0.0</td><td>1195</td><td>false</td></tr><tr><td>2025-09-02</td><td>&quot;store_009&quot;</td><td>24.65</td><td>1</td><td>11.6</td><td>1.0</td><td>703</td><td>false</td></tr><tr><td>2025-09-03</td><td>&quot;store_002&quot;</td><td>24.65</td><td>1</td><td>11.6</td><td>1.0</td><td>676</td><td>false</td></tr><tr><td>2025-08-30</td><td>&quot;store_001&quot;</td><td>128.0</td><td>2</td><td>46.75</td><td>0.0</td><td>1443</td><td>false</td></tr><tr><td>2025-09-03</td><td>&quot;store_001&quot;</td><td>24.65</td><td>1</td><td>11.6</td><td>1.0</td><td>922</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "┌────────────┬───────────┬───────┬──────────────┬────────┬──────────────┬─────────────┬────────────┐\n",
       "│ date       ┆ store_id  ┆ sales ┆ quantity_sol ┆ profit ┆ has_promotio ┆ customer_tr ┆ is_holiday │\n",
       "│ ---        ┆ ---       ┆ ---   ┆ d            ┆ ---    ┆ n            ┆ affic       ┆ ---        │\n",
       "│ date       ┆ str       ┆ f64   ┆ ---          ┆ f64    ┆ ---          ┆ ---         ┆ bool       │\n",
       "│            ┆           ┆       ┆ i64          ┆        ┆ f64          ┆ i64         ┆            │\n",
       "╞════════════╪═══════════╪═══════╪══════════════╪════════╪══════════════╪═════════════╪════════════╡\n",
       "│ 2025-08-30 ┆ store_002 ┆ 29.0  ┆ 1            ┆ 14.5   ┆ 0.0          ┆ 1195        ┆ false      │\n",
       "│ 2025-09-02 ┆ store_009 ┆ 24.65 ┆ 1            ┆ 11.6   ┆ 1.0          ┆ 703         ┆ false      │\n",
       "│ 2025-09-03 ┆ store_002 ┆ 24.65 ┆ 1            ┆ 11.6   ┆ 1.0          ┆ 676         ┆ false      │\n",
       "│ 2025-08-30 ┆ store_001 ┆ 128.0 ┆ 2            ┆ 46.75  ┆ 0.0          ┆ 1443        ┆ false      │\n",
       "│ 2025-09-03 ┆ store_001 ┆ 24.65 ┆ 1            ┆ 11.6   ┆ 1.0          ┆ 922         ┆ false      │\n",
       "└────────────┴───────────┴───────┴──────────────┴────────┴──────────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(daily_sales.head())\n",
    "store_daily_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a12706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>store_id</th><th>product_id</th><th>category</th><th>quantity_sold</th><th>sales</th><th>cost</th><th>profit</th><th>discount_percent</th><th>unit_price</th><th>has_promotion</th><th>customer_traffic</th><th>is_holiday</th></tr><tr><td>datetime[ns]</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i8</td><td>i64</td><td>bool</td></tr></thead><tbody><tr><td>2025-08-30 00:00:00</td><td>&quot;store_001&quot;</td><td>&quot;HOME_005&quot;</td><td>&quot;Home&quot;</td><td>1</td><td>39.0</td><td>23.4</td><td>15.6</td><td>0.0</td><td>39.0</td><td>0</td><td>1443</td><td>false</td></tr><tr><td>2025-08-30 00:00:00</td><td>&quot;store_001&quot;</td><td>&quot;SPRT_005&quot;</td><td>&quot;Sports&quot;</td><td>1</td><td>89.0</td><td>57.85</td><td>31.15</td><td>0.0</td><td>89.0</td><td>0</td><td>1443</td><td>false</td></tr><tr><td>2025-08-30 00:00:00</td><td>&quot;store_002&quot;</td><td>&quot;CLTH_001&quot;</td><td>&quot;Clothing&quot;</td><td>1</td><td>29.0</td><td>14.5</td><td>14.5</td><td>0.0</td><td>29.0</td><td>0</td><td>1195</td><td>false</td></tr><tr><td>2025-08-31 00:00:00</td><td>&quot;store_009&quot;</td><td>&quot;SPRT_001&quot;</td><td>&quot;Sports&quot;</td><td>1</td><td>29.0</td><td>13.05</td><td>15.95</td><td>0.0</td><td>29.0</td><td>0</td><td>1221</td><td>false</td></tr><tr><td>2025-08-31 00:00:00</td><td>&quot;store_009&quot;</td><td>&quot;CLTH_001&quot;</td><td>&quot;Clothing&quot;</td><td>1</td><td>29.0</td><td>14.5</td><td>14.5</td><td>0.0</td><td>29.0</td><td>0</td><td>1221</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "┌─────┬─────┬─────┬────────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐\n",
       "│ dat ┆ sto ┆ pro ┆ catego ┆ quant ┆ sales ┆ cost  ┆ profi ┆ disco ┆ unit_ ┆ has_p ┆ custo ┆ is_ho │\n",
       "│ e   ┆ re_ ┆ duc ┆ ry     ┆ ity_s ┆ ---   ┆ ---   ┆ t     ┆ unt_p ┆ price ┆ romot ┆ mer_t ┆ liday │\n",
       "│ --- ┆ id  ┆ t_i ┆ ---    ┆ old   ┆ f64   ┆ f64   ┆ ---   ┆ ercen ┆ ---   ┆ ion   ┆ raffi ┆ ---   │\n",
       "│ dat ┆ --- ┆ d   ┆ str    ┆ ---   ┆       ┆       ┆ f64   ┆ t     ┆ f64   ┆ ---   ┆ c     ┆ bool  │\n",
       "│ eti ┆ str ┆ --- ┆        ┆ i64   ┆       ┆       ┆       ┆ ---   ┆       ┆ i8    ┆ ---   ┆       │\n",
       "│ me[ ┆     ┆ str ┆        ┆       ┆       ┆       ┆       ┆ f64   ┆       ┆       ┆ i64   ┆       │\n",
       "│ ns] ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "╞═════╪═════╪═════╪════════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╡\n",
       "│ 202 ┆ sto ┆ HOM ┆ Home   ┆ 1     ┆ 39.0  ┆ 23.4  ┆ 15.6  ┆ 0.0   ┆ 39.0  ┆ 0     ┆ 1443  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ E_0 ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 001 ┆ 05  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 0   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ SPR ┆ Sports ┆ 1     ┆ 89.0  ┆ 57.85 ┆ 31.15 ┆ 0.0   ┆ 89.0  ┆ 0     ┆ 1443  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ T_0 ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 001 ┆ 05  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 0   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ CLT ┆ Clothi ┆ 1     ┆ 29.0  ┆ 14.5  ┆ 14.5  ┆ 0.0   ┆ 29.0  ┆ 0     ┆ 1195  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ H_0 ┆ ng     ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 002 ┆ 01  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 0   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ SPR ┆ Sports ┆ 1     ┆ 29.0  ┆ 13.05 ┆ 15.95 ┆ 0.0   ┆ 29.0  ┆ 0     ┆ 1221  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ T_0 ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 009 ┆ 01  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 1   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 202 ┆ sto ┆ CLT ┆ Clothi ┆ 1     ┆ 29.0  ┆ 14.5  ┆ 14.5  ┆ 0.0   ┆ 29.0  ┆ 0     ┆ 1221  ┆ false │\n",
       "│ 5-0 ┆ re_ ┆ H_0 ┆ ng     ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 8-3 ┆ 009 ┆ 01  ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 1   ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00: ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "│ 00  ┆     ┆     ┆        ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       ┆       │\n",
       "└─────┴─────┴─────┴────────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>store_id</th><th>sales</th><th>quantity_sold</th><th>profit</th><th>has_promotion</th><th>customer_traffic</th><th>is_holiday</th></tr><tr><td>date</td><td>str</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>bool</td></tr></thead><tbody><tr><td>2025-08-30</td><td>&quot;store_002&quot;</td><td>29.0</td><td>1</td><td>14.5</td><td>0.0</td><td>1195</td><td>false</td></tr><tr><td>2025-09-02</td><td>&quot;store_009&quot;</td><td>24.65</td><td>1</td><td>11.6</td><td>1.0</td><td>703</td><td>false</td></tr><tr><td>2025-09-03</td><td>&quot;store_002&quot;</td><td>24.65</td><td>1</td><td>11.6</td><td>1.0</td><td>676</td><td>false</td></tr><tr><td>2025-08-30</td><td>&quot;store_001&quot;</td><td>128.0</td><td>2</td><td>46.75</td><td>0.0</td><td>1443</td><td>false</td></tr><tr><td>2025-09-03</td><td>&quot;store_001&quot;</td><td>24.65</td><td>1</td><td>11.6</td><td>1.0</td><td>922</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "┌────────────┬───────────┬───────┬──────────────┬────────┬──────────────┬─────────────┬────────────┐\n",
       "│ date       ┆ store_id  ┆ sales ┆ quantity_sol ┆ profit ┆ has_promotio ┆ customer_tr ┆ is_holiday │\n",
       "│ ---        ┆ ---       ┆ ---   ┆ d            ┆ ---    ┆ n            ┆ affic       ┆ ---        │\n",
       "│ date       ┆ str       ┆ f64   ┆ ---          ┆ f64    ┆ ---          ┆ ---         ┆ bool       │\n",
       "│            ┆           ┆       ┆ i64          ┆        ┆ f64          ┆ i64         ┆            │\n",
       "╞════════════╪═══════════╪═══════╪══════════════╪════════╪══════════════╪═════════════╪════════════╡\n",
       "│ 2025-08-30 ┆ store_002 ┆ 29.0  ┆ 1            ┆ 14.5   ┆ 0.0          ┆ 1195        ┆ false      │\n",
       "│ 2025-09-02 ┆ store_009 ┆ 24.65 ┆ 1            ┆ 11.6   ┆ 1.0          ┆ 703         ┆ false      │\n",
       "│ 2025-09-03 ┆ store_002 ┆ 24.65 ┆ 1            ┆ 11.6   ┆ 1.0          ┆ 676         ┆ false      │\n",
       "│ 2025-08-30 ┆ store_001 ┆ 128.0 ┆ 2            ┆ 46.75  ┆ 0.0          ┆ 1443        ┆ false      │\n",
       "│ 2025-09-03 ┆ store_001 ┆ 24.65 ┆ 1            ┆ 11.6   ┆ 1.0          ┆ 922         ┆ false      │\n",
       "└────────────┴───────────┴───────┴──────────────┴────────┴──────────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(daily_sales.head())\n",
    "store_daily_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d1bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d2d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b122004e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0851577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ed365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pl.DataFrame = pl.DataFrame(\n",
    "    data={\n",
    "        \"id\": [1, 2, 3, 4],\n",
    "        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Bob\"],\n",
    "        \"role\": [\"Engineer\", \"Manager\", \"Engineer\", \"Manager\"],\n",
    "        \"skill\": [\"Python\", \"Leadership\", \"Python\", \"Management\"],\n",
    "        \"experience\": [5, 2, 3, 3],\n",
    "        \"age\": [30, 40, 35, 34],\n",
    "        \"target\": [1, 0, 1, 1],\n",
    "    }\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f87382",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df[\"name\"].value_counts()\n",
    "mean_target = df.group_by(\"name\").agg(pl.col(\"target\").mean())\n",
    "display(mean_target)\n",
    "display(counts[\"name\"])\n",
    "for row in counts[\"name\"]:\n",
    "    print(counts.filter(pl.col(\"name\").eq(row))[\"count\"].item())\n",
    "\n",
    "counts.filter(pl.col(\"name\").eq(\"Alice\"))[\"count\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bac96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f527771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d70434e",
   "metadata": {},
   "source": [
    "### Connect To MLFlow\n",
    "\n",
    "- Set the `tracking URI` to the MLflow server.\n",
    "    - Tracking URI requires the MLflow `server address`, `port`, `S3 endpoint URL`, and `S3 credentials`.\n",
    "    - S3 credentials include `access key`, `secret key`, and `bucket name`.\n",
    "    - `MinIO` is used as a local S3-compatible storage service.\n",
    "\n",
    "- Verify the connection by listing experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf35299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force localhost configuration and debug\n",
    "RUNNING_IN_DOCKER = False\n",
    "DEFAULT_MINIO_HOST = app_settings.AWS_S3_HOST if RUNNING_IN_DOCKER else \"minio\"\n",
    "DEFAULT_MINIO_PORT = app_settings.AWS_S3_PORT\n",
    "MINIO_ENDPOINT = app_settings.mlflow_s3_endpoint_url\n",
    "# This connects to the MLflow server with PostgreSQL backend\n",
    "MLFLOW_URI = app_settings.mlflow_tracking_uri\n",
    "AWS_KEY = app_settings.AWS_ACCESS_KEY_ID\n",
    "AWS_SECRET = app_settings.AWS_SECRET_ACCESS_KEY.get_secret_value()\n",
    "AWS_REGION = app_settings.AWS_DEFAULT_REGION\n",
    "BUCKET = app_settings.AWS_S3_BUCKET\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = app_settings.AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = MINIO_ENDPOINT\n",
    "\n",
    "print(\"=== CONFIGURATION DEBUG ===\")\n",
    "print(f\"RUNNING_IN_DOCKER: {RUNNING_IN_DOCKER}\")\n",
    "print(f\"DEFAULT_MINIO_HOST: {DEFAULT_MINIO_HOST}\")\n",
    "print(f\"MINIO_ENDPOINT: {MINIO_ENDPOINT}\")\n",
    "print(f\"MLFLOW_URI: {MLFLOW_URI}\")\n",
    "print(f\"AWS_ACCESS_KEY_ID: {AWS_KEY}\")\n",
    "print(f\"BUCKET: {BUCKET}\")\n",
    "print(f\"Environment MLFLOW_S3_ENDPOINT_URL: {MINIO_ENDPOINT}\")\n",
    "print(\"=== END CONFIGURATION DEBUG ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b689ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MLflow server connection and S3 storage\n",
    "import tempfile\n",
    "import traceback\n",
    "\n",
    "import boto3\n",
    "import mlflow\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# 1) Test S3/MinIO connection\n",
    "print(\"Testing S3/MinIO connection...\")\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=AWS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET,\n",
    "    region_name=AWS_REGION,\n",
    ")\n",
    "\n",
    "try:\n",
    "    s3.head_bucket(Bucket=BUCKET)\n",
    "    print(f\"✅ Bucket '{BUCKET}' is reachable\")\n",
    "except ClientError as e:\n",
    "    print(f\"❌ S3/MinIO connection failed: {e}\")\n",
    "\n",
    "# 2) Test MLflow server connection\n",
    "print(f\"\\nTesting MLflow server connection to {MLFLOW_URI}...\")\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "print(f\"✅ MLflow tracking URI set to: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# 3) Test that MLflow uses PostgreSQL backend (not local files)\n",
    "try:\n",
    "    # This should connect to the MLflow server which uses PostgreSQL\n",
    "    experiments = mlflow.search_experiments()\n",
    "    print(f\"✅ Connected to MLflow server. Found {len(experiments)} experiments.\")\n",
    "    print(\"✅ This confirms MLflow is using the PostgreSQL backend, not local files.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect to MLflow server: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"IMPORTANT: If MLflow server is using PostgreSQL correctly,\")\n",
    "print(\"experiments and runs will be stored in the database,\")\n",
    "print(\"and artifacts will be stored in MinIO/S3.\")\n",
    "print(\"Local 'mlruns' folders should NOT be created.\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from botocore.exceptions import ClientError\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "try:\n",
    "    mlflow.set_experiment(\"notebook_quick_test\")\n",
    "    X, y = datasets.load_diabetes(return_X_y=True)\n",
    "    model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.log_param(\"alpha\", 0.1)\n",
    "        mlflow.log_param(\"l1_ratio\", 0.5)\n",
    "        mlflow.log_metric(\"dummy_score\", model.score(X, y))\n",
    "\n",
    "        # Create a small artifact file and upload\n",
    "        with tempfile.NamedTemporaryFile(\"w\", suffix=\".txt\", delete=False) as tmp:\n",
    "            tmp.write(\"mlflow artifact test\")\n",
    "            tmp_path = tmp.name\n",
    "\n",
    "        mlflow.log_artifact(tmp_path, artifact_path=\"test_artifacts\")\n",
    "        mlflow.sklearn.log_model(model, \"model\", input_example=X[:2].tolist())\n",
    "\n",
    "        # Remove temp file after logging\n",
    "        os.remove(tmp_path)\n",
    "\n",
    "        print(\"✅ Logged run id:\", run.info.run_id)\n",
    "        print(\"✅ Experiment id:\", run.info.experiment_id)\n",
    "\n",
    "    print(\"✅ MLflow logging complete — check the UI and MinIO for artifact/model.\")\n",
    "    print(\"✅ Data stored in PostgreSQL database, artifacts in MinIO S3\")\n",
    "\n",
    "except ClientError as e:\n",
    "    # boto3 ClientError can surface during artifact upload\n",
    "    print(\"❌ Boto3 ClientError during MLflow operations:\", e)\n",
    "    print(traceback.format_exc())\n",
    "    raise\n",
    "except Exception:\n",
    "    print(\"❌ Unexpected error during MLflow logging:\")\n",
    "    print(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93578dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca7341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eceaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ded3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cyclical_features(df: pl.DataFrame, date_col: str = \"date\") -> pl.DataFrame:\n",
    "    df = df.clone()\n",
    "\n",
    "    return df.with_columns(\n",
    "        # month (convert 1-12 to 0-11 for proper cyclical encoding)\n",
    "        pl.col(date_col).dt.month().map_elements(lambda x: np.sin(2 * np.pi * (x - 1) / 12)).alias(\"month_sin\"),\n",
    "        pl.col(date_col).dt.month().map_elements(lambda x: np.cos(2 * np.pi * (x - 1) / 12)).alias(\"month_cos\"),\n",
    "        # day (Retain original values; 1-31)\n",
    "        pl.col(date_col).dt.day().map_elements(lambda x: np.sin(2 * np.pi * x / 31)).alias(\"day_sin\"),\n",
    "        pl.col(date_col).dt.day().map_elements(lambda x: np.cos(2 * np.pi * x / 31)).alias(\"day_cos\"),\n",
    "        # day of week (convert 1-7 to 0-6 for proper cyclical encoding)\n",
    "        pl.col(date_col).dt.weekday().map_elements(lambda x: np.sin(2 * np.pi * (x - 1) / 7)).alias(\"day_of_week_sin\"),\n",
    "        pl.col(date_col).dt.weekday().map_elements(lambda x: np.cos(2 * np.pi * (x - 1) / 7)).alias(\"day_of_week_cos\"),\n",
    "    )\n",
    "\n",
    "\n",
    "create_cyclical_features(temp_df, date_col=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10909e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the cyclical features to see what's wrong\n",
    "cyclical_result = create_cyclical_features(temp_df, date_col=\"date\")\n",
    "\n",
    "# Check the cyclical features\n",
    "cyclical_sample = cyclical_result.select(\n",
    "    [\n",
    "        \"date\",\n",
    "        \"day_of_week\",\n",
    "        \"day_of_week_sin\",\n",
    "        \"day_of_week_cos\",\n",
    "        \"month_sin\",\n",
    "        \"month_cos\",\n",
    "    ]\n",
    ").unique()\n",
    "\n",
    "print(\"Cyclical features sample:\")\n",
    "print(cyclical_sample)\n",
    "\n",
    "print(\"\\nLet's check the day_of_week values and corresponding sin/cos:\")\n",
    "day_check = (\n",
    "    cyclical_result.select([\"date\", \"day_of_week\", \"day_of_week_sin\", \"day_of_week_cos\"]).unique().sort(\"day_of_week\")\n",
    ")\n",
    "print(day_check)\n",
    "\n",
    "print(\"\\nIssue Analysis:\")\n",
    "print(\"day_of_week ranges from 1-7 in Polars (Monday=1, Sunday=7)\")\n",
    "print(\"But for cyclical encoding, we want values from 0 to 2π\")\n",
    "print(\"Current formula: sin(2π × day_of_week / 7)\")\n",
    "print(\"This means day 7 gives: sin(2π × 7 / 7) = sin(2π) = 0\")\n",
    "print(\"And day 1 gives: sin(2π × 1 / 7) = sin(2π/7)\")\n",
    "print(\"This creates a discontinuity between Sunday (7) and Monday (1)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a93ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "# Add include path\n",
    "sys.path.append(\"/usr/local/airflow/include\")\n",
    "\n",
    "from ml_models.train_models import ModelTrainer\n",
    "from utils.mlflow_utils import MLflowManager\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data-team\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"start_date\": datetime(2025, 7, 22),\n",
    "    \"email_on_failure\": True,\n",
    "    \"email_on_retry\": False,\n",
    "    \"email\": [\"admin@example.com\"],\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "\n",
    "@dag(\n",
    "    schedule=\"@weekly\",\n",
    "    start_date=datetime(2025, 7, 22),\n",
    "    catchup=False,\n",
    "    default_args=default_args,\n",
    "    description=\"Train sales forecasting models\",\n",
    "    tags=[\"ml\", \"training\", \"sales\"],\n",
    ")\n",
    "def sales_forecast_training():\n",
    "    @task()\n",
    "    def extract_data_task():\n",
    "        from utils.data_generator import RealisticSalesDataGenerator\n",
    "\n",
    "        data_output_dir = \"/tmp/sales_data\"\n",
    "        generator = RealisticSalesDataGenerator(start_date=\"2021-01-01\", end_date=\"2021-12-31\")\n",
    "        print(\"Generating realistic sales data...\")\n",
    "        file_paths = generator.generate_sales_data(output_dir=data_output_dir)\n",
    "        total_files = sum(len(paths) for paths in file_paths.values())\n",
    "        print(f\"Generated {total_files} files:\")\n",
    "        for data_type, paths in file_paths.items():\n",
    "            print(f\"  - {data_type}: {len(paths)} files\")\n",
    "        return {\n",
    "            \"data_output_dir\": data_output_dir,\n",
    "            \"file_paths\": file_paths,\n",
    "            \"total_files\": total_files,\n",
    "        }\n",
    "\n",
    "    @task()\n",
    "    def validate_data_task(extract_result):\n",
    "        file_paths = extract_result[\"file_paths\"]\n",
    "        total_rows = 0\n",
    "        issues_found = []\n",
    "        print(f\"Validating {len(file_paths['sales'])} sales files...\")\n",
    "        for i, sales_file in enumerate(file_paths[\"sales\"][:10]):\n",
    "            df = pd.read_parquet(sales_file)\n",
    "            if i == 0:\n",
    "                print(f\"Sales data columns: {df.columns.tolist()}\")\n",
    "            if df.empty:\n",
    "                issues_found.append(f\"Empty file: {sales_file}\")\n",
    "                continue\n",
    "            required_cols = [\n",
    "                \"date\",\n",
    "                \"store_id\",\n",
    "                \"product_id\",\n",
    "                \"quantity_sold\",\n",
    "                \"revenue\",\n",
    "            ]\n",
    "            missing_cols = set(required_cols) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                issues_found.append(f\"Missing columns in {sales_file}: {missing_cols}\")\n",
    "            total_rows += len(df)\n",
    "            if df[\"quantity_sold\"].min() < 0:\n",
    "                issues_found.append(f\"Negative quantities in {sales_file}\")\n",
    "            if df[\"revenue\"].min() < 0:\n",
    "                issues_found.append(f\"Negative revenue in {sales_file}\")\n",
    "        for data_type in [\"promotions\", \"store_events\", \"customer_traffic\"]:\n",
    "            if data_type in file_paths and file_paths[data_type]:\n",
    "                sample_file = file_paths[data_type][0]\n",
    "                df = pd.read_parquet(sample_file)\n",
    "                print(f\"{data_type} data shape: {df.shape}\")\n",
    "                print(f\"{data_type} columns: {df.columns.tolist()}\")\n",
    "        validation_summary = {\n",
    "            \"total_files_validated\": len(file_paths[\"sales\"][:10]),\n",
    "            \"total_rows\": total_rows,\n",
    "            \"issues_found\": len(issues_found),\n",
    "            \"issues\": issues_found[:5],\n",
    "        }\n",
    "        if issues_found:\n",
    "            print(f\"Validation completed with {len(issues_found)} issues:\")\n",
    "            for issue in issues_found[:5]:\n",
    "                print(f\"  - {issue}\")\n",
    "        else:\n",
    "            print(f\"Validation passed! Total rows: {total_rows}\")\n",
    "        return validation_summary\n",
    "\n",
    "    @task()\n",
    "    def train_models_task(extract_result, validation_summary):\n",
    "        file_paths = extract_result[\"file_paths\"]\n",
    "        print(\"Loading sales data from multiple files...\")\n",
    "        sales_dfs = []\n",
    "        max_files = 50\n",
    "        for i, sales_file in enumerate(file_paths[\"sales\"][:max_files]):\n",
    "            df = pd.read_parquet(sales_file)\n",
    "            sales_dfs.append(df)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Loaded {i + 1} files...\")\n",
    "        sales_df = pd.concat(sales_dfs, ignore_index=True)\n",
    "        print(f\"Combined sales data shape: {sales_df.shape}\")\n",
    "        daily_sales = (\n",
    "            sales_df.groupby([\"date\", \"store_id\", \"product_id\", \"category\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"quantity_sold\": \"sum\",\n",
    "                    \"revenue\": \"sum\",\n",
    "                    \"cost\": \"sum\",\n",
    "                    \"profit\": \"sum\",\n",
    "                    \"discount_percent\": \"mean\",\n",
    "                    \"unit_price\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        daily_sales = daily_sales.rename(columns={\"revenue\": \"sales\"})\n",
    "        if file_paths.get(\"promotions\"):\n",
    "            promo_df = pd.read_parquet(file_paths[\"promotions\"][0])\n",
    "            promo_summary = promo_df.groupby([\"date\", \"product_id\"])[\"discount_percent\"].max().reset_index()\n",
    "            promo_summary[\"has_promotion\"] = 1\n",
    "            daily_sales = daily_sales.merge(\n",
    "                promo_summary[[\"date\", \"product_id\", \"has_promotion\"]],\n",
    "                on=[\"date\", \"product_id\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            daily_sales[\"has_promotion\"] = daily_sales[\"has_promotion\"].fillna(0)\n",
    "        if file_paths.get(\"customer_traffic\"):\n",
    "            traffic_dfs = []\n",
    "            for traffic_file in file_paths[\"customer_traffic\"][:10]:\n",
    "                traffic_dfs.append(pd.read_parquet(traffic_file))\n",
    "            traffic_df = pd.concat(traffic_dfs, ignore_index=True)\n",
    "            traffic_summary = (\n",
    "                traffic_df.groupby([\"date\", \"store_id\"]).agg({\"customer_traffic\": \"sum\", \"is_holiday\": \"max\"}).reset_index()\n",
    "            )\n",
    "            daily_sales = daily_sales.merge(traffic_summary, on=[\"date\", \"store_id\"], how=\"left\")\n",
    "        print(f\"Final training data shape: {daily_sales.shape}\")\n",
    "        print(f\"Columns: {daily_sales.columns.tolist()}\")\n",
    "        trainer = ModelTrainer()\n",
    "        store_daily_sales = (\n",
    "            daily_sales.groupby([\"date\", \"store_id\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"sales\": \"sum\",\n",
    "                    \"quantity_sold\": \"sum\",\n",
    "                    \"profit\": \"sum\",\n",
    "                    \"has_promotion\": \"mean\",\n",
    "                    \"customer_traffic\": \"first\",\n",
    "                    \"is_holiday\": \"first\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        store_daily_sales[\"date\"] = pd.to_datetime(store_daily_sales[\"date\"])\n",
    "        train_df, val_df, test_df = trainer.prepare_data(\n",
    "            store_daily_sales,\n",
    "            target_col=\"sales\",\n",
    "            date_col=\"date\",\n",
    "            group_cols=[\"store_id\"],\n",
    "            categorical_cols=[\"store_id\"],\n",
    "        )\n",
    "        print(f\"Train shape: {train_df.shape}, Val shape: {val_df.shape}, Test shape: {test_df.shape}\")\n",
    "        results = trainer.train_all_models(train_df, val_df, test_df, target_col=\"sales\", use_optuna=True)\n",
    "        for model_name, model_results in results.items():\n",
    "            if \"metrics\" in model_results:\n",
    "                print(f\"\\n{model_name} metrics:\")\n",
    "                for metric, value in model_results[\"metrics\"].items():\n",
    "                    print(f\"  {metric}: {value:.4f}\")\n",
    "        print(\"\\nVisualization charts have been generated and saved to MLflow/MinIO\")\n",
    "        print(\"Charts include:\")\n",
    "        print(\"  - Model metrics comparison\")\n",
    "        print(\"  - Predictions vs actual values\")\n",
    "        print(\"  - Residuals analysis\")\n",
    "        print(\"  - Error distribution\")\n",
    "        print(\"  - Feature importance comparison\")\n",
    "        serializable_results = {}\n",
    "        for model_name, model_results in results.items():\n",
    "            serializable_results[model_name] = {\"metrics\": model_results.get(\"metrics\", {})}\n",
    "        import mlflow\n",
    "\n",
    "        current_run_id = mlflow.active_run().info.run_id if mlflow.active_run() else None\n",
    "        return {\n",
    "            \"training_results\": serializable_results,\n",
    "            \"mlflow_run_id\": current_run_id,\n",
    "        }\n",
    "\n",
    "    @task()\n",
    "    def evaluate_models_task(training_result):\n",
    "        results = training_result[\"training_results\"]\n",
    "        mlflow_manager = MLflowManager()\n",
    "        best_model_name = None\n",
    "        best_rmse = float(\"inf\")\n",
    "        for model_name, model_results in results.items():\n",
    "            if \"metrics\" in model_results and \"rmse\" in model_results[\"metrics\"]:\n",
    "                if model_results[\"metrics\"][\"rmse\"] < best_rmse:\n",
    "                    best_rmse = model_results[\"metrics\"][\"rmse\"]\n",
    "                    best_model_name = model_name\n",
    "        print(f\"Best model: {best_model_name} with RMSE: {best_rmse:.4f}\")\n",
    "        best_run = mlflow_manager.get_best_model(metric=\"rmse\", ascending=True)\n",
    "        return {\"best_model\": best_model_name, \"best_run_id\": best_run[\"run_id\"]}\n",
    "\n",
    "    @task()\n",
    "    def register_best_model_task(evaluation_result):\n",
    "        evaluation_result[\"best_model\"]\n",
    "        run_id = evaluation_result[\"best_run_id\"]\n",
    "        mlflow_manager = MLflowManager()\n",
    "        model_versions = {}\n",
    "        for model_name in [\"xgboost\", \"lightgbm\"]:\n",
    "            version = mlflow_manager.register_model(run_id, model_name, model_name)\n",
    "            model_versions[model_name] = version\n",
    "            print(f\"Registered {model_name} version: {version}\")\n",
    "        return model_versions\n",
    "\n",
    "    @task()\n",
    "    def transition_to_production_task(model_versions):\n",
    "        mlflow_manager = MLflowManager()\n",
    "        for model_name, version in model_versions.items():\n",
    "            mlflow_manager.transition_model_stage(model_name, version, \"Production\")\n",
    "            print(f\"Transitioned {model_name} v{version} to Production\")\n",
    "        return \"Models transitioned to production\"\n",
    "\n",
    "    @task()\n",
    "    def generate_performance_report_task(training_result, validation_summary):\n",
    "        results = training_result[\"training_results\"]\n",
    "        report = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_summary\": {\n",
    "                \"total_rows\": (validation_summary.get(\"total_rows\", 0) if validation_summary else 0),\n",
    "                \"files_validated\": (validation_summary.get(\"total_files_validated\", 0) if validation_summary else 0),\n",
    "                \"issues_found\": (validation_summary.get(\"issues_found\", 0) if validation_summary else 0),\n",
    "                \"issues\": (validation_summary.get(\"issues\", []) if validation_summary else []),\n",
    "            },\n",
    "            \"model_performance\": {},\n",
    "        }\n",
    "        if results:\n",
    "            for model_name, model_results in results.items():\n",
    "                if \"metrics\" in model_results:\n",
    "                    report[\"model_performance\"][model_name] = model_results[\"metrics\"]\n",
    "        import json\n",
    "\n",
    "        with open(\"/tmp/performance_report.json\", \"w\") as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        print(\"Performance report generated\")\n",
    "        print(f\"Models trained: {list(report['model_performance'].keys())}\")\n",
    "        return report\n",
    "\n",
    "    # Task dependencies using function calls\n",
    "    extract_result = extract_data_task()\n",
    "    validation_summary = validate_data_task(extract_result)\n",
    "    training_result = train_models_task(extract_result, validation_summary)\n",
    "    evaluation_result = evaluate_models_task(training_result)\n",
    "    model_versions = register_best_model_task(evaluation_result)\n",
    "    transition = transition_to_production_task(model_versions)\n",
    "    report = generate_performance_report_task(training_result, validation_summary)\n",
    "    cleanup = BashOperator(\n",
    "        task_id=\"cleanup\",\n",
    "        bash_command=\"rm -rf /tmp/sales_data /tmp/performance_report.json || true\",\n",
    "    )\n",
    "    report >> cleanup\n",
    "\n",
    "\n",
    "sales_forecast_training_dag = sales_forecast_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db62b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree .\n",
    ".\n",
    "├── airflow\n",
    "│   ├── dags\n",
    "│   │   ├── demo_dag.py\n",
    "│   │   ├── ml_dag.py\n",
    "│   │   └── simple_dag.py\n",
    "├── artifacts\n",
    "├── dags\n",
    "├── data\n",
    "│   ├── customer_traffic\n",
    "│   │   └── year=2021\n",
    "│   │       ├── month=01\n",
    "│   │       │   ├── day=01\n",
    "│   │       │   │   └── traffic_2021-01-01.parquet\n",
    "├── docker\n",
    "│   ├── Dockerfile.airflow\n",
    "│   ├── Dockerfile.mlflow\n",
    "│   └── requirements.txt\n",
    "├── docker-compose.yaml\n",
    "├── include\n",
    "│   ├── __init__.py\n",
    "│   ├── config\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── config.py\n",
    "│   │   ├── config.yaml\n",
    "│   │   └── settings.py\n",
    "│   ├── ml\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── diagnostics.py\n",
    "│   │   ├── ensemble_model.py\n",
    "│   │   ├── trainer.py\n",
    "│   │   └── visualization.py\n",
    "│   ├── schemas\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── input_schema.py\n",
    "│   └── utilities\n",
    "│       ├── __init__.py\n",
    "│       ├── data_gen.py\n",
    "│       ├── feature_engineering.py\n",
    "│       ├── mlflow_s3_utils.py\n",
    "│       ├── mlflow_utils.py\n",
    "│       ├── s3_verification.py\n",
    "│       └── service_discovery.py\n",
    "├── makefile\n",
    "├── note.py\n",
    "├── note.txt\n",
    "├── notebooks\n",
    "│   ├── 01-lab.ipynb\n",
    "├── plugins\n",
    "├── pyproject.toml\n",
    "├── README.md\n",
    "├── setup_airflow.sh\n",
    "└── uv.lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eafaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9ffe527",
   "metadata": {},
   "source": [
    "## Docker Container Import Testing\n",
    "\n",
    "When working with the Airflow containers, imports work correctly when you run Python from the right directory.\n",
    "\n",
    "### ✅ Correct way to import in Airflow containers:\n",
    "\n",
    "```bash\n",
    "# Start container shell from the correct directory\n",
    "docker compose exec airflow-worker bash\n",
    "\n",
    "# You'll be in /opt/airflow - this is the correct working directory\n",
    "pwd  # Should show: /opt/airflow\n",
    "\n",
    "# Now run Python and import\n",
    "python\n",
    "```\n",
    "\n",
    "```python\n",
    "# These imports will work correctly:\n",
    "import pandas as pd\n",
    "from include.config import app_settings\n",
    "from include.utilities.data_gen import RealisticSalesDataGenerator\n",
    "\n",
    "# Test the imports\n",
    "print(\"All imports successful!\")\n",
    "print(\"MLFLOW_HOST:\", app_settings.MLFLOW_HOST)\n",
    "gen = RealisticSalesDataGenerator(start_date=\"2025-09-01\", end_date=\"2025-09-02\", seed=42)\n",
    "print(\"Data generator created:\", type(gen))\n",
    "```\n",
    "\n",
    "### ❌ Common mistake - don't do this:\n",
    "\n",
    "```bash\n",
    "# Don't cd into the include directory first\n",
    "cd include  # This breaks imports!\n",
    "python      # Imports will fail from here\n",
    "```\n",
    "\n",
    "### Why this happens:\n",
    "\n",
    "1. Our `PYTHONPATH` is set to `/opt/airflow/include`\n",
    "2. When you run `python` from `/opt/airflow/include`, Python adds `.` (current directory) to sys.path\n",
    "3. This creates a conflict where Python tries to import `include` from within itself\n",
    "4. The solution: always run Python from `/opt/airflow` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482512b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports in Docker container (run this to verify everything works)\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "def test_docker_imports():\n",
    "    \"\"\"Test that imports work correctly in the Airflow container.\"\"\"\n",
    "\n",
    "    # Test command to run in the container\n",
    "    test_script = \"\"\"\n",
    "import sys\n",
    "import pandas as pd\n",
    "from include.config import app_settings\n",
    "from include.utilities.data_gen import RealisticSalesDataGenerator\n",
    "\n",
    "# Test results\n",
    "results = {\n",
    "    \"python_path_includes_include\": \"/opt/airflow/include\" in sys.path,\n",
    "    \"current_working_directory\": __import__(\"os\").getcwd(),\n",
    "    \"pandas_version\": pd.__version__,\n",
    "    \"mlflow_host\": app_settings.MLFLOW_HOST,\n",
    "    \"data_generator_created\": str(type(RealisticSalesDataGenerator(start_date=\"2025-09-01\", end_date=\"2025-09-02\", seed=42)))\n",
    "}\n",
    "\n",
    "import json\n",
    "print(json.dumps(results, indent=2))\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Run the test in the container\n",
    "        cmd = [\n",
    "            \"docker\",\n",
    "            \"compose\",\n",
    "            \"exec\",\n",
    "            \"-T\",\n",
    "            \"airflow-worker\",\n",
    "            \"python\",\n",
    "            \"-c\",\n",
    "            test_script,\n",
    "        ]\n",
    "\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, cwd=\"../\")\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            test_results = json.loads(result.stdout.strip())\n",
    "            print(\"✅ Docker container import test PASSED!\")\n",
    "            print(\"\\nTest Results:\")\n",
    "            for key, value in test_results.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Docker container import test FAILED!\")\n",
    "            print(\"Error output:\", result.stderr)\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to run Docker test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run the test\n",
    "test_docker_imports()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "End-to-end-Sale-Forecasting (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
